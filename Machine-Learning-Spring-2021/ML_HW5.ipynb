{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496e2686",
   "metadata": {},
   "source": [
    "# HW5: Transformer\n",
    "\n",
    "# Sequence-to-Sequence 介紹\n",
    "- 大多數常見的 seq2seq model 為 encoder-decoder model，主要由兩個部分組成，分別是 encoder 和 decoder，而這兩個部可以使用 recurrent neural network (RNN)或 transformer 來實作，主要是用來解決輸入和輸出的長度不一樣的情況\n",
    "- **Encoder** 是將一連串的輸入，如文字、影片、聲音訊號等，編碼為單個向量，這單個向量可以想像為是整個輸入的抽象表示，包含了整個輸入的資訊\n",
    "- **Decoder** 是將 encoder 輸出的單個向量逐步解碼，一次輸出一個結果，直到將最後目標輸出被產生出來為止，每次輸出會影響下一次的輸出，一般會在開頭加入 \"< BOS >\" 來表示開始解碼，會在結尾輸出 \"< EOS >\" 來表示輸出結束\n",
    "\n",
    "# 作業介紹\n",
    "- 英文翻譯中文\n",
    "  - 輸入： 一句英文 （e.g.\t\ttom is a student .） \n",
    "  - 輸出： 中文翻譯 （e.g. \t\t湯姆 是 個 學生 。）\n",
    "\n",
    "- TODO\n",
    "  - 訓練一個 RNN 模型達到 Seq2seq 翻譯\n",
    "  - 訓練一個 Transformer 大幅提升效能\n",
    "  - 實作 Back-translation 大幅提升效能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f787b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from transformers import (\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    MarianConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "\n",
    "import sentencepiece as spm\n",
    "from datasets import load_metric\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d347bc0",
   "metadata": {},
   "source": [
    "## 1. 设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639641d9",
   "metadata": {},
   "source": [
    "## 2. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ff878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # 路径\n",
    "    data_dir = './res/hw5/ted2020'\n",
    "    save_dir = './output/hw5checkpoints_hf'\n",
    "    \n",
    "    # 数据\n",
    "    src_lang = 'en'\n",
    "    tgt_lang = 'zh'\n",
    "    max_length = 128\n",
    "    vocab_size = 8000\n",
    "    \n",
    "    # 模型\n",
    "    model_type = 'custom'  # 'custom' or 'pretrained'\n",
    "    d_model = 512\n",
    "    num_encoder_layers = 6\n",
    "    num_decoder_layers = 6\n",
    "    num_attention_heads = 8\n",
    "    ffn_dim = 2048\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # 训练\n",
    "    batch_size = 32\n",
    "    gradient_accumulation_steps = 2\n",
    "    num_epochs = 30\n",
    "    learning_rate = 5e-4\n",
    "    warmup_steps = 4000\n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    # 推理\n",
    "    beam_size = 5\n",
    "    length_penalty = 1.0\n",
    "    \n",
    "    # 其他\n",
    "    num_workers = 4\n",
    "    save_steps = 1000\n",
    "    eval_steps = 1000\n",
    "    logging_steps = 100\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1acb02a",
   "metadata": {},
   "source": [
    "## 3. 数据预处理工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strQ2B(ustring):\n",
    "    \"\"\"全形转半形\"\"\"\n",
    "    ss = []\n",
    "    for s in ustring:\n",
    "        rstring = \"\"\n",
    "        for uchar in s:\n",
    "            inside_code = ord(uchar)\n",
    "            if inside_code == 12288:\n",
    "                inside_code = 32\n",
    "            elif (inside_code >= 65281 and inside_code <= 65374):\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        ss.append(rstring)\n",
    "    return ''.join(ss)\n",
    "\n",
    "def clean_text(text, lang):\n",
    "    \"\"\"清理文本\"\"\"\n",
    "    if lang == 'en':\n",
    "        text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n",
    "        text = text.replace('-', '')\n",
    "        text = re.sub('([.,;!?()\\\"])', r' \\1 ', text)\n",
    "    elif lang == 'zh':\n",
    "        text = strQ2B(text)\n",
    "        text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n",
    "        text = text.replace(' ', '')\n",
    "        text = text.replace('—', '')\n",
    "        text = text.replace('\"', '\"')\n",
    "        text = text.replace('\"', '\"')\n",
    "        text = text.replace('_', '')\n",
    "        text = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', text)\n",
    "    \n",
    "    text = ' '.join(text.strip().split())\n",
    "    return text\n",
    "\n",
    "def get_text_length(text, lang):\n",
    "    \"\"\"计算文本长度\"\"\"\n",
    "    if lang == 'zh':\n",
    "        return len(text.replace(' ', ''))\n",
    "    return len(text.split())\n",
    "\n",
    "def filter_parallel_corpus(src_file, tgt_file, output_prefix, \n",
    "                          ratio=9, max_len=1000, min_len=1):\n",
    "    \"\"\"过滤平行语料\"\"\"\n",
    "    src_out = f\"{output_prefix}.{config.src_lang}\"\n",
    "    tgt_out = f\"{output_prefix}.{config.tgt_lang}\"\n",
    "    \n",
    "    if os.path.exists(src_out) and os.path.exists(tgt_out):\n",
    "        print(f\"Filtered files already exist, skipping...\")\n",
    "        return\n",
    "    \n",
    "    with open(src_file, 'r', encoding='utf-8') as f_src, \\\n",
    "         open(tgt_file, 'r', encoding='utf-8') as f_tgt, \\\n",
    "         open(src_out, 'w', encoding='utf-8') as f_src_out, \\\n",
    "         open(tgt_out, 'w', encoding='utf-8') as f_tgt_out:\n",
    "        \n",
    "        kept = 0\n",
    "        total = 0\n",
    "        \n",
    "        for src_line, tgt_line in zip(f_src, f_tgt):\n",
    "            total += 1\n",
    "            src_line = clean_text(src_line.strip(), config.src_lang)\n",
    "            tgt_line = clean_text(tgt_line.strip(), config.tgt_lang)\n",
    "            \n",
    "            src_len = get_text_length(src_line, config.src_lang)\n",
    "            tgt_len = get_text_length(tgt_line, config.tgt_lang)\n",
    "            \n",
    "            # 过滤条件\n",
    "            if min_len > 0 and (src_len < min_len or tgt_len < min_len):\n",
    "                continue\n",
    "            if max_len > 0 and (src_len > max_len or tgt_len > max_len):\n",
    "                continue\n",
    "            if ratio > 0 and (src_len/tgt_len > ratio or tgt_len/src_len > ratio):\n",
    "                continue\n",
    "            \n",
    "            f_src_out.write(src_line + '\\n')\n",
    "            f_tgt_out.write(tgt_line + '\\n')\n",
    "            kept += 1\n",
    "        \n",
    "        print(f\"Kept {kept}/{total} sentence pairs ({kept/total*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e63fe9",
   "metadata": {},
   "source": [
    "## 4. 自定义 Tokenizer (使用 SentencePiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPMTokenizer:\n",
    "    def __init__(self, model_path):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "        self.vocab_size = self.sp.vocab_size()\n",
    "        \n",
    "        # 特殊 token\n",
    "        self.pad_token = '<pad>'\n",
    "        self.unk_token = '<unk>'\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "        \n",
    "        self.pad_token_id = self.sp.pad_id()\n",
    "        self.unk_token_id = self.sp.unk_id()\n",
    "        self.bos_token_id = self.sp.bos_id()\n",
    "        self.eos_token_id = self.sp.eos_id()\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=True, max_length=None, \n",
    "               padding=False, truncation=False, return_tensors=None):\n",
    "        \"\"\"编码文本\"\"\"\n",
    "        ids = self.sp.encode(text, out_type=int)\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            ids = [self.bos_token_id] + ids + [self.eos_token_id]\n",
    "        \n",
    "        if truncation and max_length:\n",
    "            ids = ids[:max_length]\n",
    "        \n",
    "        if padding and max_length:\n",
    "            if len(ids) < max_length:\n",
    "                ids = ids + [self.pad_token_id] * (max_length - len(ids))\n",
    "        \n",
    "        if return_tensors == 'pt':\n",
    "            ids = torch.tensor([ids])\n",
    "        \n",
    "        return {'input_ids': ids if return_tensors is None else ids}\n",
    "    \n",
    "    def decode(self, ids, skip_special_tokens=True):\n",
    "        \"\"\"解码 token ids\"\"\"\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "        \n",
    "        if skip_special_tokens:\n",
    "            ids = [id for id in ids if id not in \n",
    "                   [self.pad_token_id, self.bos_token_id, self.eos_token_id]]\n",
    "        \n",
    "        return self.sp.decode(ids)\n",
    "    \n",
    "    def batch_decode(self, ids_list, skip_special_tokens=True):\n",
    "        \"\"\"批量解码\"\"\"\n",
    "        return [self.decode(ids, skip_special_tokens) for ids in ids_list]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d30b18",
   "metadata": {},
   "source": [
    "## 5. 训练 SentencePiece 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentencepiece(input_files, model_prefix, vocab_size=8000):\n",
    "    \"\"\"训练 SentencePiece 模型\"\"\"\n",
    "    model_file = f\"{model_prefix}.model\"\n",
    "    \n",
    "    if os.path.exists(model_file):\n",
    "        print(f\"SentencePiece model already exists: {model_file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Training SentencePiece model...\")\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=','.join(input_files),\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=1.0,\n",
    "        model_type='unigram',\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3,\n",
    "        pad_piece='<pad>',\n",
    "        unk_piece='<unk>',\n",
    "        bos_piece='<s>',\n",
    "        eos_piece='</s>',\n",
    "        user_defined_symbols=[],\n",
    "    )\n",
    "    print(f\"SentencePiece model saved to {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda28d6",
   "metadata": {},
   "source": [
    "## 6. Dataset 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11707e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, tgt_file, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 读取数据\n",
    "        with open(src_file, 'r', encoding='utf-8') as f:\n",
    "            self.src_texts = [line.strip() for line in f]\n",
    "        \n",
    "        with open(tgt_file, 'r', encoding='utf-8') as f:\n",
    "            self.tgt_texts = [line.strip() for line in f]\n",
    "        \n",
    "        assert len(self.src_texts) == len(self.tgt_texts)\n",
    "        print(f\"Loaded {len(self.src_texts)} examples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_texts[idx]\n",
    "        tgt_text = self.tgt_texts[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        src_ids = self.tokenizer.sp.encode(src_text, out_type=int)\n",
    "        tgt_ids = self.tokenizer.sp.encode(tgt_text, out_type=int)\n",
    "        \n",
    "        # 添加特殊 token 和截断\n",
    "        src_ids = [self.tokenizer.bos_token_id] + src_ids + [self.tokenizer.eos_token_id]\n",
    "        tgt_ids = [self.tokenizer.bos_token_id] + tgt_ids + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        if len(src_ids) > self.max_length:\n",
    "            src_ids = src_ids[:self.max_length-1] + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        if len(tgt_ids) > self.max_length:\n",
    "            tgt_ids = tgt_ids[:self.max_length-1] + [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': src_ids,\n",
    "            'labels': tgt_ids,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, pad_token_id=0):\n",
    "    \"\"\"自定义 collate function\"\"\"\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Padding\n",
    "    max_src_len = max(len(ids) for ids in input_ids)\n",
    "    max_tgt_len = max(len(ids) for ids in labels)\n",
    "    \n",
    "    input_ids_padded = []\n",
    "    attention_mask = []\n",
    "    labels_padded = []\n",
    "    \n",
    "    for ids in input_ids:\n",
    "        padding_length = max_src_len - len(ids)\n",
    "        input_ids_padded.append(ids + [pad_token_id] * padding_length)\n",
    "        attention_mask.append([1] * len(ids) + [0] * padding_length)\n",
    "    \n",
    "    for ids in labels:\n",
    "        padding_length = max_tgt_len - len(ids)\n",
    "        # Label padding 用 -100 (PyTorch 会忽略)\n",
    "        labels_padded.append(ids + [-100] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids_padded, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "        'labels': torch.tensor(labels_padded, dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b72791",
   "metadata": {},
   "source": [
    "## 7. 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(vocab_size, config):\n",
    "    \"\"\"构建 Transformer 模型\"\"\"\n",
    "    model_config = MarianConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=config.d_model,\n",
    "        encoder_layers=config.num_encoder_layers,\n",
    "        decoder_layers=config.num_decoder_layers,\n",
    "        encoder_attention_heads=config.num_attention_heads,\n",
    "        decoder_attention_heads=config.num_attention_heads,\n",
    "        encoder_ffn_dim=config.ffn_dim,\n",
    "        decoder_ffn_dim=config.ffn_dim,\n",
    "        dropout=config.dropout,\n",
    "        attention_dropout=config.dropout,\n",
    "        activation_dropout=config.dropout,\n",
    "        max_position_embeddings=1024,\n",
    "        pad_token_id=0,\n",
    "        bos_token_id=2,\n",
    "        eos_token_id=3,\n",
    "        decoder_start_token_id=2,\n",
    "        forced_eos_token_id=3,\n",
    "    )\n",
    "    \n",
    "    model = MarianMTModel(model_config)\n",
    "    \n",
    "    # 初始化参数\n",
    "    def init_weights(module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "    \n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40d5d0",
   "metadata": {},
   "source": [
    "## 8. 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device, config):\n",
    "    \"\"\"训练一个 epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # 移动到设备\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # 混合精度训练\n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss = loss / config.gradient_accumulation_steps\n",
    "        \n",
    "        # 反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # 梯度累积\n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * config.gradient_accumulation_steps\n",
    "        progress_bar.set_postfix({'loss': loss.item() * config.gradient_accumulation_steps})\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d4a43",
   "metadata": {},
   "source": [
    "## 9. 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a858ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, tokenizer, device, config):\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # 计算 loss\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            # 生成翻译\n",
    "            generated = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config.max_length,\n",
    "                num_beams=config.beam_size,\n",
    "                length_penalty=config.length_penalty,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            # 解码\n",
    "            pred_texts = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "            ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            predictions.extend(pred_texts)\n",
    "            references.extend([[ref] for ref in ref_texts])\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # 计算 BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [[ref[0] for ref in references]])\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'bleu': bleu.score,\n",
    "        'predictions': predictions[:5],\n",
    "        'references': [ref[0] for ref in references[:5]],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe322338",
   "metadata": {},
   "source": [
    "## 10. 主训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeaafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"Machine Translation Training with Hugging Face Transformers\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. 数据预处理\n",
    "    print(\"\\n[1/6] Data Preprocessing...\")\n",
    "    prefix = Path(config.data_dir)\n",
    "    \n",
    "    # 过滤数据\n",
    "    filter_parallel_corpus(\n",
    "        f\"{prefix}/train_dev.raw.en\",\n",
    "        f\"{prefix}/train_dev.raw.zh\",\n",
    "        f\"{prefix}/train_dev.clean\"\n",
    "    )\n",
    "    \n",
    "    # 切分训练集和验证集\n",
    "    train_src = f\"{prefix}/train.{config.src_lang}\"\n",
    "    train_tgt = f\"{prefix}/train.{config.tgt_lang}\"\n",
    "    valid_src = f\"{prefix}/valid.{config.src_lang}\"\n",
    "    valid_tgt = f\"{prefix}/valid.{config.tgt_lang}\"\n",
    "    \n",
    "    if not os.path.exists(train_src):\n",
    "        print(\"Splitting train/valid...\")\n",
    "        with open(f\"{prefix}/train_dev.clean.{config.src_lang}\", 'r') as f_src, \\\n",
    "             open(f\"{prefix}/train_dev.clean.{config.tgt_lang}\", 'r') as f_tgt:\n",
    "            \n",
    "            src_lines = f_src.readlines()\n",
    "            tgt_lines = f_tgt.readlines()\n",
    "            \n",
    "            split_idx = int(len(src_lines) * 0.99)\n",
    "            \n",
    "            with open(train_src, 'w') as f:\n",
    "                f.writelines(src_lines[:split_idx])\n",
    "            with open(train_tgt, 'w') as f:\n",
    "                f.writelines(tgt_lines[:split_idx])\n",
    "            with open(valid_src, 'w') as f:\n",
    "                f.writelines(src_lines[split_idx:])\n",
    "            with open(valid_tgt, 'w') as f:\n",
    "                f.writelines(tgt_lines[split_idx:])\n",
    "    \n",
    "    # 2. 训练 SentencePiece\n",
    "    print(\"\\n[2/6] Training SentencePiece...\")\n",
    "    spm_prefix = f\"{prefix}/spm{config.vocab_size}\"\n",
    "    train_sentencepiece(\n",
    "        [train_src, train_tgt, valid_src, valid_tgt],\n",
    "        spm_prefix,\n",
    "        config.vocab_size\n",
    "    )\n",
    "    \n",
    "    # 3. 加载 Tokenizer\n",
    "    print(\"\\n[3/6] Loading Tokenizer...\")\n",
    "    tokenizer = SPMTokenizer(f\"{spm_prefix}.model\")\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "    \n",
    "    # 4. 创建 Dataset 和 DataLoader\n",
    "    print(\"\\n[4/6] Creating Datasets...\")\n",
    "    train_dataset = TranslationDataset(train_src, train_tgt, tokenizer, config.max_length)\n",
    "    valid_dataset = TranslationDataset(valid_src, valid_tgt, tokenizer, config.max_length)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        collate_fn=lambda x: collate_fn(x, tokenizer.pad_token_id),\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        collate_fn=lambda x: collate_fn(x, tokenizer.pad_token_id),\n",
    "    )\n",
    "    \n",
    "    # 5. 构建模型\n",
    "    print(\"\\n[5/6] Building Model...\")\n",
    "    model = build_transformer_model(len(tokenizer), config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {num_params:,}\")\n",
    "    print(f\"Trainable parameters: {num_trainable:,}\")\n",
    "    \n",
    "    # 6. 优化器和调度器\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-9,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    num_training_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config.warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # 7. 训练循环\n",
    "    print(\"\\n[6/6] Training...\")\n",
    "    best_bleu = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # 训练\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, device, config)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # 评估\n",
    "        eval_results = evaluate(model, valid_loader, tokenizer, device, config)\n",
    "        print(f\"Valid Loss: {eval_results['loss']:.4f}\")\n",
    "        print(f\"Valid BLEU: {eval_results['bleu']:.2f}\")\n",
    "        \n",
    "        # 显示示例\n",
    "        print(\"\\nExample translations:\")\n",
    "        for i, (pred, ref) in enumerate(zip(eval_results['predictions'], eval_results['references'])):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"  Prediction: {pred}\")\n",
    "            print(f\"  Reference:  {ref}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if eval_results['bleu'] > best_bleu:\n",
    "            best_bleu = eval_results['bleu']\n",
    "            model.save_pretrained(f\"{config.save_dir}/best_model\")\n",
    "            tokenizer.sp.save(f\"{config.save_dir}/tokenizer.model\")\n",
    "            print(f\"✓ Saved best model (BLEU: {best_bleu:.2f})\")\n",
    "        \n",
    "        # 保存检查点\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.save_pretrained(f\"{config.save_dir}/checkpoint-epoch-{epoch+1}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Training completed! Best BLEU: {best_bleu:.2f}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16607fc0",
   "metadata": {},
   "source": [
    "## 11. 推理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbcac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model_path, tokenizer_path, test_src, output_file, config):\n",
    "    \"\"\"生成测试集预测\"\"\"\n",
    "    print(\"Loading model for inference...\")\n",
    "    \n",
    "    # 加载模型和 tokenizer\n",
    "    model = MarianMTModel.from_pretrained(model_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = SPMTokenizer(tokenizer_path)\n",
    "    \n",
    "    # 读取测试数据\n",
    "    with open(test_src, 'r', encoding='utf-8') as f:\n",
    "        test_texts = [line.strip() for line in f]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(test_texts), config.batch_size), desc=\"Generating\"):\n",
    "            batch_texts = test_texts[i:i + config.batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            input_ids_list = []\n",
    "            for text in batch_texts:\n",
    "                ids = tokenizer.sp.encode(text, out_type=int)\n",
    "                ids = [tokenizer.bos_token_id] + ids + [tokenizer.eos_token_id]\n",
    "                input_ids_list.append(ids)\n",
    "            \n",
    "            # Padding\n",
    "            max_len = max(len(ids) for ids in input_ids_list)\n",
    "            input_ids_padded = []\n",
    "            attention_mask = []\n",
    "            \n",
    "            for ids in input_ids_list:\n",
    "                padding_length = max_len - len(ids)\n",
    "                input_ids_padded.append(ids + [tokenizer.pad_token_id] * padding_length)\n",
    "                attention_mask.append([1] * len(ids) + [0] * padding_length)\n",
    "            \n",
    "            input_ids = torch.tensor(input_ids_padded, dtype=torch.long).to(device)\n",
    "            attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(device)\n",
    "            \n",
    "            # Generate\n",
    "            generated = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config.max_length,\n",
    "                num_beams=config.beam_size,\n",
    "                length_penalty=config.length_penalty,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "            \n",
    "            # Decode\n",
    "            batch_predictions = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "            predictions.extend(batch_predictions)\n",
    "    \n",
    "    # 保存预测\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for pred in predictions:\n",
    "            f.write(pred + '\\n')\n",
    "    \n",
    "    print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623cadd6",
   "metadata": {},
   "source": [
    "## 12. 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 训练模型\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成测试集预测\n",
    "generate_predictions(\n",
    "    model_path=f\"{config.save_dir}/best_model\",\n",
    "    tokenizer_path=f\"{config.save_dir}/tokenizer.model\",\n",
    "    test_src=f\"{config.data_dir}/test.raw.{config.src_lang}\",\n",
    "    output_file=\"./prediction.txt\",\n",
    "    config=config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
