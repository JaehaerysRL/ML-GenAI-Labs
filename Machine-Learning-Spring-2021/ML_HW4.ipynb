{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54032d48",
   "metadata": {},
   "source": [
    "# HW4: Self-Attention\n",
    "\n",
    "Speaker Classification using Transformer with Keras 3.0\n",
    "\n",
    "Optimized version with improved architecture and training pipeline\n",
    "\n",
    "Original dataset is **Voxceleb1**. We randomly select 600 speakers from Voxceleb1.Then preprocess the raw waveforms into mel-spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8521f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HomeworkID\n",
    "HOMEWORK_ID = \"hw4\"\n",
    "# Res Directory\n",
    "RES_DIR = f\"res/{HOMEWORK_ID}/\"\n",
    "# Output Directory\n",
    "OUTPUT_DIR = f\"output/{HOMEWORK_ID}/\"\n",
    "# Random Seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd30851b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning libraries\n",
    "import torch\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5a3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "def get_device():\n",
    "    ''' Returns the available device: GPU if available, else CPU '''\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_random_seed(seed_value=RANDOM_SEED):\n",
    "    ''' Sets the random seed for reproducibility '''\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        # torch.backends.cudnn.deterministic = True # å¼ºåˆ¶CuDNNä½¿ç”¨ç¡®å®šæ€§ç®—æ³•\n",
    "        # torch.backends.cudnn.benchmark = False # å…³é—­CuDNNçš„è‡ªåŠ¨ä¼˜åŒ–åŠŸèƒ½\n",
    "\n",
    "def plot_learning_curve(loss_record: dict, title: str):\n",
    "    \"\"\"Plots the learning curve\"\"\"\n",
    "    total_epochs = len(loss_record[\"loss\"])\n",
    "    x_train = np.arange(1, total_epochs + 1)\n",
    "    \n",
    "    if len(loss_record[\"val_loss\"]) > 0:\n",
    "        x_val = x_train[:: max(1, len(x_train) // len(loss_record[\"val_loss\"]))]\n",
    "        x_val = x_val[:len(loss_record[\"val_loss\"])]\n",
    "    else:\n",
    "        x_val = []\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x_train, loss_record[\"loss\"], c=\"tab:red\", label=\"Training Loss\", linewidth=2)\n",
    "    if len(x_val) > 0:\n",
    "        plt.plot(x_val, loss_record[\"val_loss\"], c=\"tab:blue\", label=\"Validation Loss\", linewidth=2)\n",
    "    plt.xlabel(\"Epochs\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.title(\"Loss Curve\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if \"accuracy\" in loss_record:\n",
    "        plt.plot(x_train, loss_record[\"accuracy\"], c=\"tab:green\", label=\"Training Accuracy\", linewidth=2)\n",
    "    if \"val_accuracy\" in loss_record and len(x_val) > 0:\n",
    "        plt.plot(x_val, loss_record[\"val_accuracy\"], c=\"tab:orange\", label=\"Validation Accuracy\", linewidth=2)\n",
    "    plt.xlabel(\"Epochs\", fontsize=12)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "    plt.title(\"Accuracy Curve\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f\"{title.replace(' ', '_')}.png\"), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"æ¸…ç†å†…å­˜\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "787b5ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Using device: cuda\n",
      "PyTorch version: 2.9.1+cu130\n",
      "Keras Version: 3.13.0\n",
      "Current Backend: torch\n",
      "========================================\n",
      "Random seed set to: 42\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# GPU Checking\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Using device: {get_device()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print(f\"Current Backend: {keras.backend.backend()}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_random_seed(RANDOM_SEED)\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# Make sure OUTPUT_DIR exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2813ab",
   "metadata": {},
   "source": [
    "## Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737bb12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for speaker classification training\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, segment_len=128):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.segment_len = segment_len\n",
    "        \n",
    "        # Load mappings\n",
    "        with open(self.data_dir / \"mapping.json\") as f:\n",
    "            mapping = json.load(f)\n",
    "        self.speaker2id = mapping[\"speaker2id\"]\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(self.data_dir / \"metadata.json\") as f:\n",
    "            metadata = json.load(f)[\"speakers\"]\n",
    "        \n",
    "        self.speaker_num = len(metadata.keys())\n",
    "        self.data = []\n",
    "        \n",
    "        for speaker in metadata.keys():\n",
    "            for utterance in metadata[speaker]:\n",
    "                self.data.append([\n",
    "                    utterance[\"feature_path\"], \n",
    "                    self.speaker2id[speaker]\n",
    "                ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feat_path, speaker = self.data[index]\n",
    "        mel = torch.load(self.data_dir / feat_path)\n",
    "        \n",
    "        # Segment or pad\n",
    "        if len(mel) > self.segment_len:\n",
    "            start = random.randint(0, len(mel) - self.segment_len)\n",
    "            mel = mel[start:start + self.segment_len]\n",
    "        \n",
    "        mel = torch.FloatTensor(mel)\n",
    "        speaker = torch.LongTensor([speaker])\n",
    "        \n",
    "        return mel, speaker\n",
    "    \n",
    "    def get_speaker_number(self):\n",
    "        return self.speaker_num\n",
    "\n",
    "\n",
    "class InferenceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for inference\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "        with open(self.data_dir / \"testdata.json\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        self.data = metadata[\"utterances\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        utterance = self.data[index]\n",
    "        feat_path = utterance[\"feature_path\"]\n",
    "        mel = torch.load(self.data_dir / feat_path)\n",
    "        \n",
    "        return feat_path, torch.FloatTensor(mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6078189",
   "metadata": {},
   "source": [
    "## Collate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8296685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"Collate function for training\"\"\"\n",
    "    mels, speakers = zip(*batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    mels = torch.nn.utils.rnn.pad_sequence(\n",
    "        mels, batch_first=True, padding_value=-20.0\n",
    "    )\n",
    "    speakers = torch.cat(speakers)\n",
    "    \n",
    "    return mels.numpy(), speakers.numpy()\n",
    "\n",
    "\n",
    "def inference_collate_batch(batch):\n",
    "    \"\"\"Collate function for inference\"\"\"\n",
    "    feat_paths, mels = zip(*batch)\n",
    "    mels = torch.nn.utils.rnn.pad_sequence(\n",
    "        mels, batch_first=True, padding_value=-20.0\n",
    "    )\n",
    "    return feat_paths, mels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f51a2",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader Adapter for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4fe84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataLoaderAdapter(keras.utils.PyDataset):\n",
    "    \"\"\"Adapter to use PyTorch DataLoader with Keras\"\"\"\n",
    "    \n",
    "    def __init__(self, dataloader, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.iterator is None:\n",
    "            self.iterator = iter(self.dataloader)\n",
    "        \n",
    "        try:\n",
    "            batch = next(self.iterator)\n",
    "        except StopIteration:\n",
    "            self.iterator = iter(self.dataloader)\n",
    "            batch = next(self.iterator)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a4158",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38804793",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class TransformerSpeakerClassifier(keras.Model):\n",
    "    \"\"\"\n",
    "    Improved Transformer-based Speaker Classifier\n",
    "    \n",
    "    Features:\n",
    "    - Multi-head self-attention\n",
    "    - Layer normalization\n",
    "    - Dropout for regularization\n",
    "    - Statistics pooling (mean + std)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_mels=40, \n",
    "        d_model=256, \n",
    "        n_heads=4, \n",
    "        n_layers=4,\n",
    "        ff_dim=1024,\n",
    "        n_spks=600, \n",
    "        dropout=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.n_mels = n_mels\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.ff_dim = ff_dim\n",
    "        self.n_spks = n_spks\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = keras.layers.Dense(d_model, name=\"input_projection\")\n",
    "        self.input_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.input_dropout = keras.layers.Dropout(dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.attention_layers = []\n",
    "        self.attention_dropout = []\n",
    "        self.attention_norm = []\n",
    "        \n",
    "        self.ffn_layers = []\n",
    "        self.ffn_dropout = []\n",
    "        self.ffn_norm = []\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            # Multi-head attention\n",
    "            self.attention_layers.append(\n",
    "                keras.layers.MultiHeadAttention(\n",
    "                    num_heads=n_heads,\n",
    "                    key_dim=d_model // n_heads,\n",
    "                    dropout=dropout,\n",
    "                    name=f\"mha_{i}\"\n",
    "                )\n",
    "            )\n",
    "            self.attention_dropout.append(keras.layers.Dropout(dropout))\n",
    "            self.attention_norm.append(keras.layers.LayerNormalization(epsilon=1e-6))\n",
    "            \n",
    "            # Feed-forward network\n",
    "            self.ffn_layers.append(\n",
    "                keras.Sequential([\n",
    "                    keras.layers.Dense(ff_dim, activation='relu'),\n",
    "                    keras.layers.Dropout(dropout),\n",
    "                    keras.layers.Dense(d_model),\n",
    "                ], name=f\"ffn_{i}\")\n",
    "            )\n",
    "            self.ffn_dropout.append(keras.layers.Dropout(dropout))\n",
    "            self.ffn_norm.append(keras.layers.LayerNormalization(epsilon=1e-6))\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = keras.Sequential([\n",
    "            keras.layers.Dense(d_model, activation='relu'),\n",
    "            keras.layers.Dropout(dropout),\n",
    "            keras.layers.Dense(d_model // 2, activation='relu'),\n",
    "            keras.layers.Dropout(dropout),\n",
    "            keras.layers.Dense(n_spks)\n",
    "        ], name=\"classifier\")\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # inputs: (batch_size, seq_len, n_mels)\n",
    "        x = self.input_projection(inputs)\n",
    "        x = self.input_norm(x)\n",
    "        x = self.input_dropout(x, training=training)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        for i in range(self.n_layers):\n",
    "            # Multi-head attention with residual\n",
    "            attn_output = self.attention_layers[i](x, x, training=training)\n",
    "            attn_output = self.attention_dropout[i](attn_output, training=training)\n",
    "            x = self.attention_norm[i](x + attn_output)\n",
    "            \n",
    "            # Feed-forward network with residual\n",
    "            ffn_output = self.ffn_layers[i](x, training=training)\n",
    "            ffn_output = self.ffn_dropout[i](ffn_output, training=training)\n",
    "            x = self.ffn_norm[i](x + ffn_output)\n",
    "        \n",
    "        # Statistics pooling: concatenate mean and std\n",
    "        mean = keras.ops.mean(x, axis=1)\n",
    "        std = keras.ops.std(x, axis=1)\n",
    "        stats = keras.ops.concatenate([mean, std], axis=-1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(stats, training=training)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"n_mels\": self.n_mels,\n",
    "            \"d_model\": self.d_model,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"n_spks\": self.n_spks,\n",
    "            \"dropout\": self.dropout_rate,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460f136",
   "metadata": {},
   "source": [
    "## Custom Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069f7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineDecay(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Learning rate schedule with warmup and cosine decay\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_lr, warmup_steps, total_steps, min_lr=1e-6):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = keras.ops.cast(step, \"float32\")\n",
    "        warmup_steps = keras.ops.cast(self.warmup_steps, \"float32\")\n",
    "        total_steps = keras.ops.cast(self.total_steps, \"float32\")\n",
    "        \n",
    "        # Linear warmup\n",
    "        warmup_lr = self.initial_lr * step / warmup_steps\n",
    "        \n",
    "        # Cosine decay\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        cosine_decay = 0.5 * (1.0 + keras.ops.cos(np.pi * progress))\n",
    "        decay_lr = self.min_lr + (self.initial_lr - self.min_lr) * cosine_decay\n",
    "        \n",
    "        # Choose based on step\n",
    "        lr = keras.ops.where(step < warmup_steps, warmup_lr, decay_lr)\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_lr\": self.initial_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"min_lr\": self.min_lr,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560221f5",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9888d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(data_dir, batch_size=32, n_workers=8):\n",
    "    \"\"\"Create training and validation datasets\"\"\"\n",
    "    \n",
    "    dataset = SpeakerDataset(data_dir)\n",
    "    speaker_num = dataset.get_speaker_number()\n",
    "    \n",
    "    # Split dataset (90% train, 10% validation)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_workers,\n",
    "        collate_fn=collate_batch,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        persistent_workers=True if n_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=n_workers,\n",
    "        collate_fn=collate_batch,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        persistent_workers=True if n_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, speaker_num\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    \"\"\"Main training function using Keras fit()\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Starting Training with Keras 3.0 (PyTorch Backend)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader, speaker_num = create_datasets(\n",
    "        config[\"data_dir\"],\n",
    "        config[\"batch_size\"],\n",
    "        config[\"n_workers\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Dataset Info:\")\n",
    "    print(f\"  Number of speakers: {speaker_num}\")\n",
    "    print(f\"  Training samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"  Validation samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"  Training batches: {len(train_loader)}\")\n",
    "    print(f\"  Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = TransformerSpeakerClassifier(\n",
    "        n_mels=40,\n",
    "        d_model=config.get(\"d_model\", 256),\n",
    "        n_heads=config.get(\"n_heads\", 4),\n",
    "        n_layers=config.get(\"n_layers\", 4),\n",
    "        ff_dim=config.get(\"ff_dim\", 1024),\n",
    "        n_spks=speaker_num,\n",
    "        dropout=config.get(\"dropout\", 0.1)\n",
    "    )\n",
    "    \n",
    "    # Build model by calling it once\n",
    "    dummy_input = np.random.randn(1, 128, 40).astype(np.float32)\n",
    "    _ = model(dummy_input, training=False)\n",
    "    \n",
    "    print(f\"\\nğŸ—ï¸  Model Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_schedule = WarmupCosineDecay(\n",
    "        initial_lr=config[\"learning_rate\"],\n",
    "        warmup_steps=config[\"warmup_steps\"],\n",
    "        total_steps=config[\"total_steps\"],\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay=config.get(\"weight_decay\", 0.01)\n",
    "        ),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top5_accuracy')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=config[\"save_path\"],\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=config.get(\"early_stopping_patience\", 10),\n",
    "            mode='max',\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.CSVLogger(\n",
    "            'training_log.csv',\n",
    "            append=False\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Calculate epochs\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    epochs = (config[\"total_steps\"] + steps_per_epoch - 1) // steps_per_epoch\n",
    "    \n",
    "    print(f\"\\nâš™ï¸  Training Configuration:\")\n",
    "    print(f\"  Batch size: {config['batch_size']}\")\n",
    "    print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "    print(f\"  Warmup steps: {config['warmup_steps']}\")\n",
    "    print(f\"  Total steps: {config['total_steps']}\")\n",
    "    print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"  Total epochs: {epochs}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸš€ Training Started...\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Train using Keras fit()\n",
    "    history = model.fit(\n",
    "        train_loader,\n",
    "        validation_data=val_loader,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… Training Completed!\")\n",
    "    print(f\"  Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2fc01",
   "metadata": {},
   "source": [
    "## Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca74f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(config):\n",
    "    \"\"\"Inference function using Keras predict()\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ”® Starting Inference\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load mapping\n",
    "    with open(Path(config[\"data_dir\"]) / \"mapping.json\") as f:\n",
    "        mapping = json.load(f)\n",
    "    \n",
    "    speaker_num = len(mapping[\"id2speaker\"])\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\nğŸ“‚ Loading model from {config['model_path']}...\")\n",
    "    model = keras.saving.load_model(config[\"model_path\"])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = InferenceDataset(config[\"data_dir\"])\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.get(\"inference_batch_size\", 64),\n",
    "        shuffle=False,\n",
    "        num_workers=config[\"n_workers\"],\n",
    "        collate_fn=inference_collate_batch,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"  Number of test samples: {len(dataset)}\")\n",
    "    print(f\"  Inference batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Collect all data\n",
    "    all_feat_paths = []\n",
    "    all_mels = []\n",
    "    \n",
    "    print(\"\\nğŸ“¦ Loading test data...\")\n",
    "    for feat_paths, mels in dataloader:\n",
    "        all_feat_paths.extend(feat_paths)\n",
    "        all_mels.append(mels)\n",
    "    \n",
    "    all_mels = np.concatenate(all_mels, axis=0)\n",
    "    \n",
    "    # Predict using Keras predict()\n",
    "    print(\"\\nğŸ¯ Running inference...\")\n",
    "    logits = model.predict(all_mels, batch_size=config.get(\"inference_batch_size\", 64), verbose=1)\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Prepare results\n",
    "    print(\"\\nğŸ’¾ Preparing submission file...\")\n",
    "    results = [[\"Id\", \"Category\"]]\n",
    "    for feat_path, pred in zip(all_feat_paths, preds):\n",
    "        results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n",
    "    \n",
    "    # Save results\n",
    "    with open(config[\"output_path\"], 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"\\nâœ… Results saved to {config['output_path']}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7df5a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ba3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Data\n",
    "    \"data_dir\": RES_DIR,\n",
    "    \n",
    "    # Model architecture\n",
    "    \"d_model\": 256,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 4,\n",
    "    \"ff_dim\": 1024,\n",
    "    \"dropout\": 0.1,\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"total_steps\": 70000,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \n",
    "    # System\n",
    "    \"n_workers\": 8,\n",
    "    \n",
    "    # Paths\n",
    "    \"save_path\": OUTPUT_DIR + \"best_model_keras.keras\",\n",
    "    \"model_path\": OUTPUT_DIR + \"best_model_keras.keras\",\n",
    "    \"output_path\": OUTPUT_DIR + \"submission.csv\",\n",
    "    \n",
    "    # Inference\n",
    "    \"inference_batch_size\": 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸµ Speaker Classification with Keras 3.0 + PyTorch Backend\n",
      "================================================================================\n",
      "\n",
      "PHASE 1: Training\n",
      "================================================================================\n",
      "================================================================================\n",
      "Starting Training with Keras 3.0 (PyTorch Backend)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Dataset Info:\n",
      "  Number of speakers: 600\n",
      "  Training samples: 62494\n",
      "  Validation samples: 6944\n",
      "  Training batches: 1952\n",
      "  Validation batches: 217\n",
      "\n",
      "ğŸ—ï¸  Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_speaker_classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer_speaker_classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_projection (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization             â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mha_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_1           â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ffn_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_2           â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mha_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_3           â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ffn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_4           â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mha_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_5           â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ffn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_6           â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mha_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_7           â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ffn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_8           â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ classifier (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span>)               â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">241,624</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_projection (\u001b[38;5;33mDense\u001b[0m)        â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚        \u001b[38;5;34m10,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization             â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mha_0 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚       \u001b[38;5;34m263,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_1           â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ffn_0 (\u001b[38;5;33mSequential\u001b[0m)              â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚       \u001b[38;5;34m525,568\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_2           â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mha_1 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚       \u001b[38;5;34m263,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_3           â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ffn_1 (\u001b[38;5;33mSequential\u001b[0m)              â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚       \u001b[38;5;34m525,568\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_4           â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mha_2 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚       \u001b[38;5;34m263,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_5           â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ffn_2 (\u001b[38;5;33mSequential\u001b[0m)              â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚       \u001b[38;5;34m525,568\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_6           â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mha_3 (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚       \u001b[38;5;34m263,168\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_7           â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ffn_3 (\u001b[38;5;33mSequential\u001b[0m)              â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚       \u001b[38;5;34m525,568\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization_8           â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)          â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ classifier (\u001b[38;5;33mSequential\u001b[0m)         â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m600\u001b[0m)               â”‚       \u001b[38;5;34m241,624\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,411,672</span> (13.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,411,672\u001b[0m (13.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,411,672</span> (13.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,411,672\u001b[0m (13.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™ï¸  Training Configuration:\n",
      "  Batch size: 32\n",
      "  Learning rate: 0.001\n",
      "  Warmup steps: 1000\n",
      "  Total steps: 70000\n",
      "  Steps per epoch: 1952\n",
      "  Total epochs: 36\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ Training Started...\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸµ Speaker Classification with Keras 3.0 + PyTorch Backend\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPHASE 1: Training\")\n",
    "print(\"=\" * 80)\n",
    "model, history = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(history.history, title=\"Speaker_Classification_Training_Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c61cb",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5515632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2: Inference\")\n",
    "print(\"=\" * 80)\n",
    "predictions = inference(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d843ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== é¢„æµ‹ç»Ÿè®¡ =====\n",
    "max_confidences = np.max(predictions, axis=1)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(f\"\\né¢„æµ‹ç½®ä¿¡åº¦ç»Ÿè®¡:\")\n",
    "print(f\"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(max_confidences):.4f}\")\n",
    "print(f\"  ä¸­ä½æ•°ç½®ä¿¡åº¦: {np.median(max_confidences):.4f}\")\n",
    "print(f\"  æœ€å°ç½®ä¿¡åº¦: {np.min(max_confidences):.4f}\")\n",
    "print(f\"  æœ€å¤§ç½®ä¿¡åº¦: {np.max(max_confidences):.4f}\")\n",
    "\n",
    "print(f\"\\né¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ:\")\n",
    "label_counts = np.bincount(predicted_labels, minlength=11)\n",
    "for i, count in enumerate(label_counts):\n",
    "    print(f\"  ç±»åˆ« {i}: {count} ({count/len(predictions)*100:.1f}%)\")\n",
    "\n",
    "# ===== å¯è§†åŒ– =====\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
    "axes[0].hist(max_confidences, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.mean(max_confidences), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean: {np.mean(max_confidences):.3f}')\n",
    "axes[0].set_xlabel(\"Prediction Confidence\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[0].set_title(\"Distribution of Prediction Confidences\", fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ ‡ç­¾åˆ†å¸ƒ\n",
    "axes[1].bar(range(11), label_counts, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel(\"Class Label\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Count\", fontsize=12)\n",
    "axes[1].set_title(\"Distribution of Predicted Labels\", fontsize=14)\n",
    "axes[1].set_xticks(range(11))\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"prediction_analysis.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ è®­ç»ƒå’Œé¢„æµ‹å®Œæˆï¼\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ All Done!\")\n",
    "print(\"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
