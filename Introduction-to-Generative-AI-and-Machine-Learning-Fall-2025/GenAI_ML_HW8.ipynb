{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034e1587",
   "metadata": {},
   "source": [
    "# Fine-tuning Leads to Forgetting\n",
    "\n",
    "This notebook is for GenAI-ML 2025 Homework 8, focusing on the problem of fine-tuning leading to forgetting. The goal is to fine-tune a model using the GSM8K dataset while observing the effects on previously learned knowledge about safeness.  \n",
    "ç ”ç©¶æ ¸å¿ƒä¸ºå¾®è°ƒå¼•å‘é—å¿˜è¿™ä¸€é—®é¢˜ã€‚å®žéªŒç›®æ ‡æ˜¯åŸºäºŽ GSM8K æ•°æ®é›†å¯¹æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶è§‚å¯Ÿè¯¥è¿‡ç¨‹å¯¹æ¨¡åž‹æ­¤å‰ä¹ å¾—çš„å®‰å…¨ç›¸å…³çŸ¥è¯†æ‰€äº§ç”Ÿçš„å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b2b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, # imports the model for causal language modeling\n",
    "    AutoTokenizer, # imports the tokenizer for the model\n",
    "    BitsAndBytesConfig, # imports the configuration for using bitsandbytes\n",
    "    pipeline # imports the pipeline for text generation\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, # imports the configuration for LoRA\n",
    "    get_peft_model, # imports the function to get the PEFT model\n",
    "    PeftModel # imports the PEFT model\n",
    ")\n",
    "from datasets import Dataset # Imports the Dataset class from the datasets library\n",
    "from trl import SFTConfig, SFTTrainer # Imports the SFTConfig and SFTTrainer classes from the trl library\n",
    "from tqdm import tqdm # Imports the tqdm library for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6aec224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.1+cu130\n"
     ]
    }
   ],
   "source": [
    "# Device checking\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e66ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# èµ„æºç›®å½•\n",
    "RES_DIR = \"res/hw8/\"\n",
    "# è¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR = \"output/hw8/\"\n",
    "\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0   0     0   0     0     0     0  --:--:-- --:--:-- --:--:--     0\n",
      "  0     0   0     0   0     0     0     0  --:--:-- --:--:-- --:--:--     0\n",
      "  0     0   0     0   0     0     0     0  --:--:--  0:00:01 --:--:--     0\n",
      "  0  4068k   0 16166   0     0  6498     0   0:10:41  0:00:02  0:10:39  6500\n",
      "  0  4068k   0 32550   0     0  9349     0   0:07:25  0:00:03  0:07:22  9350\n",
      "  0  4068k   0 32550   0     0  7407     0   0:09:22  0:00:04  0:09:18  7407\n",
      "  1  4068k   1 48934   0     0  8290     0   0:08:22  0:00:05  0:08:17  9060\n",
      "  1  4068k   1 48934   0     0  7080     0   0:09:48  0:00:06  0:09:42  9030\n",
      "  1  4068k   1 48934   0     0  6175     0   0:11:14  0:00:07  0:11:07  6027\n",
      "  1  4068k   1 48934   0     0  5478     0   0:12:40  0:00:08  0:12:32  3006\n",
      "  1  4068k   1 48934   0     0  4919     0   0:14:06  0:00:09  0:13:57  2951\n",
      "  1  4068k   1 48934   0     0  4465     0   0:15:33  0:00:10  0:15:23     0\n",
      "  1  4068k   1 48934   0     0  4088     0   0:16:59  0:00:11  0:16:48     0\n",
      "  1  4068k   1 48934   0     0  3769     0   0:18:25  0:00:12  0:18:13     0\n",
      "  1  4068k   1 48934   0     0  3496     0   0:19:51  0:00:13  0:19:38     0\n",
      "  1  4068k   1 48934   0     0  3261     0   0:21:17  0:00:15  0:21:02     0\n",
      "  1  4068k   1 48934   0     0  3055     0   0:22:43  0:00:16  0:22:27     0\n",
      "  1  4068k   1 48934   0     0  2873     0   0:24:10  0:00:17  0:23:53     0\n",
      "  1  4068k   1 48934   0     0  2712     0   0:25:36  0:00:18  0:25:18     0\n",
      "  1  4068k   1 48934   0     0  2568     0   0:27:02  0:00:19  0:26:43     0\n",
      "  1  4068k   1 48934   0     0  2439     0   0:28:28  0:00:20  0:28:08     0\n",
      "  1  4068k   1 48934   0     0  2322     0   0:29:54  0:00:21  0:29:33     0\n",
      "  1  4068k   1 48934   0     0  2216     0   0:31:20  0:00:22  0:30:58     0\n",
      "  1  4068k   1 48934   0     0  2119     0   0:32:46  0:00:23  0:32:23     0\n",
      "  1  4068k   1 48934   0     0  2062     0   0:33:40  0:00:23  0:33:17     0\n",
      "  1  4068k   1 65318   0     0  2593     0   0:26:46  0:00:25  0:26:21  3195\n",
      "  1  4068k   1 65318   0     0  2569     0   0:27:01  0:00:25  0:26:36  3768\n",
      "  1  4068k   1 65318   0     0  2475     0   0:28:03  0:00:26  0:27:37  3804\n",
      "  1  4068k   1 81702   0     0  2958     0   0:23:28  0:00:27  0:23:01  7243\n",
      "  1  4068k   1 81702   0     0  2854     0   0:24:19  0:00:28  0:23:51  6687\n",
      "  2  4068k   2 98086   0     0  3337     0   0:20:48  0:00:29  0:20:19  7801\n",
      "  2  4068k   2 98086   0     0  3197     0   0:21:43  0:00:30  0:21:13  6232\n",
      "  2  4068k   2 98086   0     0  3096     0   0:22:25  0:00:31  0:21:54  6189\n",
      "  2  4068k   2 114470   0     0  3455     0   0:20:05  0:00:33  0:19:32  5944\n",
      "  2  4068k   2 114470   0     0  3352     0   0:20:42  0:00:34  0:20:08  5940\n",
      "  2  4068k   2 114470   0     0  3256     0   0:21:19  0:00:35  0:20:44  2843\n",
      "  2  4068k   2 114470   0     0  3167     0   0:21:55  0:00:36  0:21:19  3001\n",
      "  2  4068k   2 114470   0     0  3125     0   0:22:13  0:00:36  0:21:37  3311\n",
      "  3  4068k   3 130854   0     0  3456     0   0:20:05  0:00:37  0:19:28  3461\n",
      "  3  4068k   3 130854   0     0  3367     0   0:20:37  0:00:38  0:19:59  3468\n",
      "  3  4068k   3 147238   0     0  3654     0   0:19:00  0:00:40  0:18:20  6376\n",
      "  3  4068k   3 147238   0     0  3633     0   0:19:06  0:00:40  0:18:26  7460\n",
      "  3  4068k   3 163622   0     0  3942     0   0:17:36  0:00:41  0:16:55 10074\n",
      "  3  4068k   3 163622   0     0  3798     0   0:18:16  0:00:43  0:17:33  6279\n",
      "  3  4068k   3 163622   0     0  3715     0   0:18:41  0:00:44  0:17:57  6338\n",
      "  4  4068k   4 180006   0     0  4018     0   0:17:16  0:00:44  0:16:32  7276\n",
      "  4  4068k   4 180006   0     0  3912     0   0:17:44  0:00:46  0:16:58  5978\n",
      "  4  4068k   4 196390   0     0  4226     0   0:16:25  0:00:46  0:15:39  6601\n",
      "  4  4068k   4 196390   0     0  4110     0   0:16:53  0:00:47  0:16:06  6964\n",
      "  5  4068k   5 212774   0     0  4381     0   0:15:50  0:00:48  0:15:02 10864\n",
      "  5  4068k   5 212774   0     0  4278     0   0:16:13  0:00:49  0:15:24  6635\n",
      "  5  4068k   5 212774   0     0  4161     0   0:16:41  0:00:51  0:15:50  6395\n",
      "  5  4068k   5 229158   0     0  4460     0   0:15:34  0:00:51  0:14:43  6672\n",
      "  5  4068k   5 229158   0     0  4333     0   0:16:01  0:00:52  0:15:09  6426\n",
      "  5  4068k   5 229158   0     0  4277     0   0:16:14  0:00:53  0:15:21  3265\n",
      "  5  4068k   5 245542   0     0  4503     0   0:15:25  0:00:54  0:14:31  6829\n",
      "  5  4068k   5 245542   0     0  4423     0   0:15:41  0:00:55  0:14:46  7479\n",
      "  6  4068k   6 261926   0     0  4648     0   0:14:56  0:00:56  0:14:00  6594\n",
      "  6  4068k   6 261926   0     0  4551     0   0:15:15  0:00:57  0:14:18  7013\n",
      "  6  4068k   6 278310   0     0  4756     0   0:14:35  0:00:58  0:13:37  9947\n",
      "  6  4068k   6 278310   0     0  4677     0   0:14:50  0:00:59  0:13:51  6594\n",
      "  7  4068k   7 294694   0     0  4873     0   0:14:14  0:01:00  0:13:14  9915\n",
      "  7  4068k   7 311078   0     0  5053     0   0:13:44  0:01:01  0:12:43  9437\n",
      "  7  4068k   7 311078   0     0  4939     0   0:14:03  0:01:02  0:13:01  9056\n",
      "  7  4068k   7 311078   0     0  4900     0   0:14:10  0:01:03  0:13:07  6606\n",
      "  7  4068k   7 327462   0     0  5047     0   0:13:45  0:01:04  0:12:41  9132\n",
      "  8  4068k   8 343846   0     0  5237     0   0:13:15  0:01:05  0:12:10  9492\n",
      "  8  4068k   8 343846   0     0  5178     0   0:13:24  0:01:06  0:12:18  6763\n",
      "  8  4068k   8 343846   0     0  5087     0   0:13:38  0:01:07  0:12:31  7114\n",
      "  8  4068k   8 360230   0     0  5269     0   0:13:10  0:01:08  0:12:02 10061\n",
      "  8  4068k   8 360230   0     0  5162     0   0:13:27  0:01:09  0:12:18  6687\n",
      "  8  4068k   8 360230   0     0  5099     0   0:13:37  0:01:10  0:12:27  3283\n",
      "  8  4068k   8 360230   0     0  5011     0   0:13:51  0:01:11  0:12:40  2985\n",
      "  8  4068k   8 360230   0     0  4941     0   0:14:03  0:01:12  0:12:51  3082\n",
      "  8  4068k   8 360230   0     0  4873     0   0:14:14  0:01:13  0:13:01     0\n",
      "  8  4068k   8 360230   0     0  4807     0   0:14:26  0:01:14  0:13:12     0\n",
      "  8  4068k   8 360230   0     0  4744     0   0:14:38  0:01:15  0:13:23     0\n",
      "  8  4068k   8 360230   0     0  4681     0   0:14:50  0:01:16  0:13:34     0\n",
      "  8  4068k   8 360230   0     0  4620     0   0:15:01  0:01:17  0:13:44     0\n",
      "  8  4068k   8 360230   0     0  4561     0   0:15:13  0:01:18  0:13:55     0\n",
      "  8  4068k   8 360230   0     0  4504     0   0:15:25  0:01:19  0:14:06     0\n",
      "  8  4068k   8 360230   0     0  4447     0   0:15:36  0:01:20  0:14:16     0\n",
      "  8  4068k   8 360230   0     0  4392     0   0:15:48  0:01:22  0:14:26     0\n",
      "  8  4068k   8 360230   0     0  4339     0   0:16:00  0:01:23  0:14:37     0\n",
      "  8  4068k   8 360230   0     0  4287     0   0:16:11  0:01:24  0:14:47     0\n",
      "  8  4068k   8 360230   0     0  4236     0   0:16:23  0:01:25  0:14:58     0\n",
      "  8  4068k   8 360230   0     0  4186     0   0:16:35  0:01:26  0:15:09     0\n",
      "  8  4068k   8 360230   0     0  4138     0   0:16:46  0:01:27  0:15:19     0\n",
      "  8  4068k   8 360230   0     0  4090     0   0:16:58  0:01:28  0:15:30     0\n",
      "  8  4068k   8 360230   0     0  4044     0   0:17:10  0:01:29  0:15:41     0\n",
      "  8  4068k   8 360230   0     0  3998     0   0:17:22  0:01:30  0:15:52     0\n",
      "  8  4068k   8 360230   0     0  3954     0   0:17:33  0:01:31  0:16:02     0\n",
      "  8  4068k   8 360230   0     0  3910     0   0:17:45  0:01:32  0:16:13     0\n",
      "  8  4068k   8 360230   0     0  3868     0   0:17:57  0:01:33  0:16:24     0\n",
      "  8  4068k   8 360230   0     0  3826     0   0:18:08  0:01:34  0:16:34     0\n",
      "  8  4068k   8 360230   0     0  3786     0   0:18:20  0:01:35  0:16:45     0\n",
      "  8  4068k   8 360230   0     0  3746     0   0:18:32  0:01:36  0:16:56     0\n",
      "  8  4068k   8 360230   0     0  3707     0   0:18:43  0:01:37  0:17:06     0\n",
      "  8  4068k   8 360230   0     0  3669     0   0:18:55  0:01:38  0:17:17     0\n",
      "  8  4068k   8 360230   0     0  3631     0   0:19:07  0:01:39  0:17:28     0\n",
      "  8  4068k   8 360230   0     0  3595     0   0:19:18  0:01:40  0:17:38     0\n",
      "  8  4068k   8 360230   0     0  3559     0   0:19:30  0:01:41  0:17:49     0\n",
      "  8  4068k   8 360230   0     0  3524     0   0:19:42  0:01:42  0:18:00     0\n",
      "  8  4068k   8 360230   0     0  3489     0   0:19:54  0:01:43  0:18:11     0\n",
      "  8  4068k   8 360230   0     0  3456     0   0:20:05  0:01:44  0:18:21     0\n",
      "  8  4068k   8 360230   0     0  3422     0   0:20:17  0:01:45  0:18:32     0\n",
      "  8  4068k   8 360230   0     0  3390     0   0:20:28  0:01:46  0:18:42     0\n",
      "  8  4068k   8 360230   0     0  3358     0   0:20:40  0:01:47  0:18:53     0\n",
      "  8  4068k   8 360230   0     0  3327     0   0:20:52  0:01:48  0:19:04     0\n",
      "  8  4068k   8 360230   0     0  3296     0   0:21:04  0:01:49  0:19:15     0\n",
      "  8  4068k   8 360230   0     0  3266     0   0:21:15  0:01:50  0:19:25     0\n",
      "  8  4068k   8 360230   0     0  3236     0   0:21:27  0:01:51  0:19:36     0\n",
      "  8  4068k   8 360230   0     0  3207     0   0:21:39  0:01:52  0:19:47     0\n",
      "  8  4068k   8 360230   0     0  3179     0   0:21:50  0:01:53  0:19:57     0\n",
      "  8  4068k   8 360230   0     0  3151     0   0:22:02  0:01:54  0:20:08     0\n",
      "  8  4068k   8 360230   0     0  3123     0   0:22:14  0:01:55  0:20:19     0\n",
      "  8  4068k   8 360230   0     0  3096     0   0:22:25  0:01:56  0:20:29     0\n",
      "  8  4068k   8 360230   0     0  3069     0   0:22:37  0:01:57  0:20:40     0\n",
      "  8  4068k   8 360230   0     0  3043     0   0:22:49  0:01:58  0:20:51     0\n",
      "  8  4068k   8 360230   0     0  3017     0   0:23:00  0:01:59  0:21:01     0\n",
      "  8  4068k   8 360230   0     0  2992     0   0:23:12  0:02:00  0:21:12     0\n",
      "  8  4068k   8 360230   0     0  2967     0   0:23:24  0:02:01  0:21:23     0\n",
      "  8  4068k   8 360230   0     0  2943     0   0:23:35  0:02:02  0:21:33     0\n",
      "  8  4068k   8 360230   0     0  2918     0   0:23:47  0:02:03  0:21:44     0\n",
      "  8  4068k   8 360230   0     0  2895     0   0:23:59  0:02:04  0:21:55     0\n",
      "  8  4068k   8 360230   0     0  2871     0   0:24:11  0:02:05  0:22:06     0\n",
      "  8  4068k   8 360230   0     0  2848     0   0:24:22  0:02:06  0:22:16     0\n",
      "  8  4068k   8 360230   0     0  2826     0   0:24:34  0:02:07  0:22:27     0\n",
      "  8  4068k   8 360230   0     0  2804     0   0:24:45  0:02:08  0:22:37     0\n",
      "  8  4068k   8 360230   0     0  2782     0   0:24:57  0:02:09  0:22:48     0\n",
      "  9  4068k   9 376614   0     0  2886     0   0:24:03  0:02:10  0:21:53  3239\n",
      "  9  4068k   9 392998   0     0  2988     0   0:23:14  0:02:11  0:21:03  6466\n",
      "  9  4068k   9 392998   0     0  2965     0   0:23:25  0:02:12  0:21:13  6482\n",
      "  9  4068k   9 392998   0     0  2926     0   0:23:43  0:02:14  0:21:29  5631\n",
      "  9  4068k   9 409382   0     0  3044     0   0:22:48  0:02:14  0:20:34  9816\n",
      "  9  4068k   9 409382   0     0  3021     0   0:22:59  0:02:15  0:20:44  6570\n",
      " 10  4068k  10 425766   0     0  3108     0   0:22:20  0:02:16  0:20:04  6000\n",
      " 10  4068k  10 442150   0     0  3210     0   0:21:37  0:02:17  0:19:20  9421\n",
      " 10  4068k  10 442150   0     0  3187     0   0:21:47  0:02:18  0:19:29 11032\n",
      " 10  4068k  10 442150   0     0  3166     0   0:21:55  0:02:19  0:19:36  6334\n",
      " 10  4068k  10 442150   0     0  3131     0   0:22:10  0:02:21  0:19:49  5750\n",
      " 10  4068k  10 442150   0     0  3119     0   0:22:15  0:02:21  0:19:54  3444\n",
      " 11  4068k  11 458534   0     0  3217     0   0:21:35  0:02:22  0:19:13  3418\n",
      " 11  4068k  11 458534   0     0  3194     0   0:21:44  0:02:23  0:19:21  3399\n",
      " 11  4068k  11 474918   0     0  3285     0   0:21:08  0:02:24  0:18:44  6658\n",
      " 11  4068k  11 474918   0     0  3256     0   0:21:19  0:02:25  0:18:54  7039\n",
      " 11  4068k  11 491302   0     0  3352     0   0:20:42  0:02:26  0:18:16 10178\n",
      " 11  4068k  11 491302   0     0  3352     0   0:20:42  0:02:26  0:18:16  8108\n",
      "curl: (56) schannel: server closed abruptly (missing close_notify)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0   0     0   0     0     0     0  --:--:-- --:--:-- --:--:--     0\n",
      "  0     0   0     0   0     0     0     0  --:--:--  0:00:01 --:--:--     0\n",
      " 21 74663  21 16169   0     0  8827     0   0:00:08  0:00:01  0:00:07  8830\n",
      " 21 74663  21 16169   0     0  5318     0   0:00:14  0:00:03  0:00:11  5322\n",
      " 43 74663  43 32553   0     0  8441     0   0:00:08  0:00:03  0:00:05  8442\n",
      " 43 74663  43 32553   0     0  6416     0   0:00:11  0:00:05  0:00:06  6419\n",
      " 65 74663  65 48937   0     0  8395     0   0:00:08  0:00:05  0:00:03 10214\n",
      " 65 74663  65 48937   0     0  6949     0   0:00:10  0:00:07  0:00:03  6289\n",
      " 65 74663  65 48937   0     0  6109     0   0:00:12  0:00:08  0:00:04  6593\n",
      " 87 74663  87 65321   0     0  7066     0   0:00:10  0:00:09  0:00:01  6082\n",
      " 87 74663  87 65321   0     0  6256     0   0:00:11  0:00:10  0:00:01  6105\n",
      "100 74663 100 74663   0     0  6992     0   0:00:10  0:00:10 --:--:--  5306\n"
     ]
    }
   ],
   "source": [
    "# Download Dataset\n",
    "!curl -L -o {RES_DIR}gsm8k_train.jsonl -k https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train.jsonl\n",
    "!curl -L -o {RES_DIR}gsm8k_train_self-instruct.jsonl -k https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train_self-instruct.jsonl\n",
    "!curl -L -o {RES_DIR}gsm8k_test_public.jsonl -k https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_public.jsonl\n",
    "!curl -L -o {RES_DIR}gsm8k_test_private.jsonl -k https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_private.jsonl\n",
    "!curl -L -o {RES_DIR}ailuminate_test.csv -k https://www.csie.ntu.edu.tw/~b10902031/ailuminate_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce91b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility in training in pytorch and hf transformers\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fe245",
   "metadata": {},
   "source": [
    "## LLM Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278147cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model & Tokenizer\n",
    "\n",
    "sft_model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "sft_bnb_config = BitsAndBytesConfig( # Configuration for using bitsandbytes\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "sft_model = AutoModelForCausalLM.from_pretrained( # Loads the pre-trained model\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    "    quantization_config=sft_bnb_config,\n",
    "    dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained( # Loads the tokenizer for the model\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    ")\n",
    "sft_tokenizer.model_max_length = 10000\n",
    "sft_tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Adds a special token for padding\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(sft_model, peft_config).to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efddd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Formatting Functions\n",
    "\n",
    "def load_jsonlines(file_name: str):\n",
    "    f = open(file_name, 'r')\n",
    "    return [json.loads(line) for line in f]\n",
    "\n",
    "def nshot_chats(nshot_data: list, n: int, question: str, answer: any, mode: str) -> dict: # Function to create n-shot chats\n",
    "    if mode not in ['train', 'test']:\n",
    "        raise AssertionError('Undefined Mode!!!')\n",
    "\n",
    "    chats = []\n",
    "    # TODO: Use fixed few-shot examples\n",
    "    for qna in random.sample(nshot_data, n): # Samples n examples from the n-shot data\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'Q: {qna[\"question\"]}' # Creates a user message with the question\n",
    "            }\n",
    "        )\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': f'A: {qna[\"answer\"]}' # Creates an assistant message with the answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    chats.append(\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Q: {question} Let\\'s think step by step. At the end, you MUST write the answer as an integer after \\'####\\'.' # Creates a user message with the question and instructions\n",
    "        }\n",
    "    )\n",
    "    if mode == 'train':\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': f'A: {answer}' # Creates an assistant message with the answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return chats # Returns the list of chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f113c5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatted_gsm8k filtered: kept 2491/7473 longest examples using fields=('text',).\n"
     ]
    }
   ],
   "source": [
    "# Format GSM8K Data for Fine-tuning\n",
    "\n",
    "# Filter GSM8K by Length\n",
    "gsm8k_train = load_jsonlines(os.path.join(RES_DIR, 'gsm8k_train.jsonl')) # Loads the GSM8K training data\n",
    "\n",
    "formatted_gsm8k = []\n",
    "# TRAIN_N_SHOT = 1 # TODO: Give model more examples\n",
    "TRAIN_N_SHOT = 8\n",
    "for qna in gsm8k_train: # Iterates over the GSM8K training data\n",
    "    chats = nshot_chats(nshot_data=gsm8k_train, n=TRAIN_N_SHOT, question=qna['question'], answer=qna['answer'], mode='train') # Creates n-shot chats for the current example\n",
    "    train_sample = sft_tokenizer.apply_chat_template(chats, tokenize=False) # Applies the chat template to the chats\n",
    "    train_sample = train_sample[train_sample.index(\"<|eot_id|>\") + len(\"<|eot_id|>\"):] # Remove Cutting Knowledge Date in prompt template\n",
    "    formatted_gsm8k.append( # Appends the formatted example to the list\n",
    "        {\n",
    "            'text': train_sample # Adds the text of the example\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "formatted_gsm8k = Dataset.from_list(formatted_gsm8k) # Creates a dataset from the list of formatted examples\n",
    "\n",
    "# Sample 1/3 of the longest data. Please do not modify this part.\n",
    "# Keep the longest 1/3 of `formatted_gsm8k` by letter count\n",
    "PORTION = 1/3  # change this if needed\n",
    "\n",
    "def _letters(s):\n",
    "    s = \"\" if s is None else (s if isinstance(s, str) else str(s))\n",
    "    return sum(1 for ch in s if ch.isalpha())\n",
    "\n",
    "# Choose fields: prefer 'text' if present, else fall back to ('question','answer')\n",
    "cols = getattr(formatted_gsm8k, \"column_names\", None) or []\n",
    "FIELDS = (\"text\",) if \"text\" in cols else (\"question\", \"answer\")\n",
    "\n",
    "n = len(formatted_gsm8k)\n",
    "k = max(1, int(round(n * PORTION)))\n",
    "\n",
    "# Compute lengths and take top-k indices\n",
    "lengths = []\n",
    "for i in range(n):\n",
    "    ex = formatted_gsm8k[i]  # dict-like\n",
    "    lengths.append(sum(_letters(ex.get(f, \"\")) for f in FIELDS))\n",
    "\n",
    "top_idx = sorted(range(n), key=lambda i: lengths[i], reverse=False)[:k] #modified to shortest 1/3\n",
    "formatted_gsm8k = formatted_gsm8k.select(top_idx)\n",
    "\n",
    "print(f\"formatted_gsm8k filtered: kept {k}/{n} longest examples using fields={FIELDS}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33ab71b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28227ec89d904220a04ed3434a9cdfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/2491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e753052306634459872f1a74a3f1c9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d908305dc53498c9202d8855cf0e66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2491 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer\n",
    "training_arguments = SFTConfig( # Configuration for the SFT trainer\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    output_dir=os.path.join(OUTPUT_DIR, 'sft'),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1, # TODO: If you use fixed few-shot examples, increase epoch\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,\n",
    "    lr_scheduler_type='linear',\n",
    "    # learning_rate=3e-4, # TODO: Decrease learning rate\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    dataset_text_field='text',\n",
    "    report_to='none',\n",
    ")\n",
    "trainer = SFTTrainer( # Creates the SFT trainer\n",
    "    model=peft_model,\n",
    "    train_dataset=formatted_gsm8k,\n",
    "    processing_class=sft_tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f990f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128256}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='623' max='623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [623/623 14:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>1.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>1.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>1.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>1.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>1.025000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=623, training_loss=1.0657319241886536, metrics={'train_runtime': 859.6669, 'train_samples_per_second': 2.898, 'train_steps_per_second': 0.725, 'total_flos': 1.4979942610440192e+16, 'train_loss': 1.0657319241886536})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cbb71",
   "metadata": {},
   "source": [
    "## LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e780fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\peft\\tuners\\tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Adapter Checkpoint\n",
    "generator = pipeline( # Creates a text generation pipeline\n",
    "    'text-generation',\n",
    "    model=sft_model,\n",
    "    tokenizer=sft_tokenizer,\n",
    "    pad_token_id=sft_tokenizer.eos_token_id,\n",
    "    max_new_tokens=256, # TODO: Increase max_new_tokens for longer output\n",
    "    # TODO: Use greedy decoding strategy\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "adapter_path = os.path.join(OUTPUT_DIR, \"sft/checkpoint-567\")\n",
    "pipeline.model = PeftModel.from_pretrained( # Loads the adapter checkpoint\n",
    "    sft_model,\n",
    "    adapter_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipeline.model.to(dtype=torch.bfloat16, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e047d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 5080 Laptop GPU bf16_supported: True\n",
      "First param dtype: torch.bfloat16\n",
      "# of float32 nn.Linear modules: 0\n",
      "Sample (up to 20): []\n",
      "input_embeddings.weight: torch.bfloat16\n",
      "output_embeddings(lm_head).weight: torch.bfloat16\n",
      "LoRA float32 params (first 20): []\n"
     ]
    }
   ],
   "source": [
    "m = pipeline.model  # or your variable holding the PEFT-wrapped model\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0), \"bf16_supported:\", torch.cuda.is_bf16_supported())\n",
    "print(\"First param dtype:\", next(m.parameters()).dtype)\n",
    "\n",
    "# Count float32 linears and list suspicious ones\n",
    "f32_modules = []\n",
    "for name, mod in m.named_modules():\n",
    "    if isinstance(mod, torch.nn.Linear):\n",
    "        if getattr(mod, \"weight\", None) is not None and mod.weight.dtype == torch.float32:\n",
    "            f32_modules.append(name)\n",
    "\n",
    "print(f\"# of float32 nn.Linear modules: {len(f32_modules)}\")\n",
    "print(\"Sample (up to 20):\", f32_modules[:20])\n",
    "\n",
    "# Check embeddings and lm_head explicitly\n",
    "if hasattr(m, \"get_input_embeddings\") and m.get_input_embeddings() is not None:\n",
    "    print(\"input_embeddings.weight:\", m.get_input_embeddings().weight.dtype)\n",
    "if hasattr(m, \"get_output_embeddings\") and m.get_output_embeddings() is not None:\n",
    "    print(\"output_embeddings(lm_head).weight:\", m.get_output_embeddings().weight.dtype)\n",
    "\n",
    "# Check LoRA params explicitly\n",
    "lora_f32 = [n for n,p in m.named_parameters() if \"lora_\" in n and p.dtype == torch.float32]\n",
    "print(\"LoRA float32 params (first 20):\", lora_f32[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48021b",
   "metadata": {},
   "source": [
    "## GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de93f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSM8K\n",
    "def get_response(chats: list): # Function to get the response from the model\n",
    "    gen_text = generator(chats)[0]  # First return sequence\n",
    "    return gen_text['generated_text'][-1]['content'] # Returns the content of the last generated text\n",
    "\n",
    "def extract_ans_from_response(answer: str): # Function to extract the answer from the response\n",
    "    answer = answer.split('####')[-1].strip() # Splits the answer by '####' and takes the last part\n",
    "\n",
    "    for remove_char in [',', '$', '%', 'g']: # Removes unwanted characters from the answer\n",
    "        answer = answer.replace(remove_char, '')\n",
    "\n",
    "    return answer # Returns the extracted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1856ae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GSM8K Public Test Data Evaluation:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [02:18<04:31,  4.05s/it, Current Accuracy = 0.242]"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: unknown error\nSearch for `cudaErrorUnknown' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, qna \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gsm8k_test_public): \u001b[38;5;66;03m# Iterates over the public test data\u001b[39;00m\n\u001b[32m     13\u001b[39m     messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m], answer=\u001b[38;5;28;01mNone\u001b[39;00m, mode=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Creates n-shot chats for the current example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     response = \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Gets the response from the model\u001b[39;00m\n\u001b[32m     16\u001b[39m     pred_ans = extract_ans_from_response(response) \u001b[38;5;66;03m# Extracts the predicted answer from the response\u001b[39;00m\n\u001b[32m     17\u001b[39m     true_ans = extract_ans_from_response(qna[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;66;03m# Extracts the true answer from the example\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mget_response\u001b[39m\u001b[34m(chats)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(chats: \u001b[38;5;28mlist\u001b[39m): \u001b[38;5;66;03m# Function to get the response from the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     gen_text = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchats\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# First return sequence\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gen_text[\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\pipelines\\text_generation.py:325\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    327\u001b[39m         chats = (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# ðŸˆ ðŸˆ ðŸˆ\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\pipelines\\base.py:1467\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1460\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1461\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1464\u001b[39m         )\n\u001b[32m   1465\u001b[39m     )\n\u001b[32m   1466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\pipelines\\base.py:1474\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1473\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1475\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\pipelines\\base.py:1374\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1373\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\pipelines\\text_generation.py:432\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    430\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    435\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\generation\\utils.py:2803\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2800\u001b[39m next_token_logits = outputs.logits[:, -\u001b[32m1\u001b[39m, :].to(copy=\u001b[38;5;28;01mTrue\u001b[39;00m, dtype=torch.float32, device=input_ids.device)\n\u001b[32m   2802\u001b[39m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2803\u001b[39m next_token_scores = \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2805\u001b[39m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[32m   2806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\generation\\logits_process.py:93\u001b[39m, in \u001b[36mLogitsProcessorList.__call__\u001b[39m\u001b[34m(self, input_ids, scores, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         scores = processor(input_ids, scores, **kwargs)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         scores = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\generation\\logits_process.py:579\u001b[39m, in \u001b[36mTopKLogitsWarper.__call__\u001b[39m\u001b[34m(self, input_ids, scores)\u001b[39m\n\u001b[32m    577\u001b[39m top_k = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.top_k, scores.size(-\u001b[32m1\u001b[39m))  \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# Remove all tokens with a probability less than the last token of the top-k\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m indices_to_remove = scores < \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][..., -\u001b[32m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    580\u001b[39m scores_processed = scores.masked_fill(indices_to_remove, \u001b[38;5;28mself\u001b[39m.filter_value)\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores_processed\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: unknown error\nSearch for `cudaErrorUnknown' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "gsm8k_predictions = []\n",
    "TEST_N_SHOT = 4 # TODO: give model more examples\n",
    "\n",
    "gsm8k_test_public = load_jsonlines(os.path.join(RES_DIR,'gsm8k_test_public.jsonl')) # Loads the GSM8K public test data\n",
    "gsm8k_test_public = gsm8k_test_public[0:100] # We use only 100 of the original 13\n",
    "gsm8k_total = len(gsm8k_test_public) # Gets the total number of examples in the public test data\n",
    "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Public Test Data Evaluation', postfix='Current Accuracy = 0.000') # Creates a progress bar for the public test data evaluation\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i, qna in enumerate(gsm8k_test_public): # Iterates over the public test data\n",
    "\n",
    "    messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
    "    response = get_response(messages) # Gets the response from the model\n",
    "\n",
    "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
    "    true_ans = extract_ans_from_response(qna[\"answer\"]) # Extracts the true answer from the example\n",
    "    if pred_ans == true_ans: # Checks if the predicted answer is correct\n",
    "        correct += 1 # Increments the correct count if the prediction is correct\n",
    "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
    "\n",
    "    gsm8k_progress_bar.set_postfix_str(f'Current Accuracy = {correct/(i+1):.3f}') # Updates the progress bar with the current accuracy\n",
    "    gsm8k_progress_bar.update() # Updates the progress bar\n",
    "\n",
    "gsm8k_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'GSM8K Public Test Data Evaluation Complete, Total Accuracy: {correct/gsm8k_total:.3f}') # Prints the total accuracy on the public test data\n",
    "\n",
    "gsm8k_test_private = load_jsonlines(os.path.join(RES_DIR,'gsm8k_test_private.jsonl')) # Loads the GSM8K private test data\n",
    "gsm8k_test_private = gsm8k_test_private[0:100]\n",
    "gsm8k_total = len(gsm8k_test_private) # Gets the total number of examples in the private test data\n",
    "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Private Test Data Inference') # Creates a progress bar for the private test data evaluation\n",
    "\n",
    "for i, qna in enumerate(gsm8k_test_private): # Iterates over the private test data\n",
    "\n",
    "    messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
    "    response = get_response(messages) # Gets the response from the model\n",
    "\n",
    "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
    "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
    "\n",
    "    gsm8k_progress_bar.update() # Updates the progress bar\n",
    "\n",
    "gsm8k_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'GSM8K Private Test Data Inference Complete') # Prints a message indicating that the private test data evaluation is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1a8e4",
   "metadata": {},
   "source": [
    "## AILuminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "105ac2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_name: str):\n",
    "    csvfile = open(file_name)\n",
    "    rows = csv.DictReader(csvfile)\n",
    "    questions = []\n",
    "    for row in rows:\n",
    "        questions.append(row['prompt_text'])\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9af2064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AILuminate Test Data Evaluation:   0%|          | 0/80 [00:16<?, ?it/s]\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "AILuminate Test Data Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [16:51<00:00, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIluminate Test Data Evaluation Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ailuminate_predictions = []\n",
    "\n",
    "ailuminate_test = load_csv(os.path.join(RES_DIR, 'ailuminate_test.csv')) # Loads the AILuminate test data\n",
    "ailuminate_public = ailuminate_test[0:40]\n",
    "ailuminate_private = ailuminate_test[120:160]\n",
    "ailuminate_test = ailuminate_public + ailuminate_private\n",
    "ailuminate_total = len(ailuminate_test) # Gets the total number of examples in the AILuminate test data\n",
    "ailuminate_progress_bar = tqdm(total=ailuminate_total, desc='AILuminate Test Data Evaluation') # Creates a progress bar for the AILuminate test data evaluation\n",
    "\n",
    "for i, question in enumerate(ailuminate_test): # Iterates over the AILuminate test data\n",
    "\n",
    "    message = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': question\n",
    "        }\n",
    "    ]\n",
    "    response = get_response(message) # Gets the response from the model\n",
    "    ailuminate_predictions.append(response) # Appends the response to the list of predictions\n",
    "\n",
    "    ailuminate_progress_bar.update() # Updates the progress bar\n",
    "ailuminate_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'AIluminate Test Data Evaluation Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bd64302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Submission File\n",
    "submission_file = os.path.join(OUTPUT_DIR, 'submission.txt')\n",
    "with open(file=submission_file, mode='w') as f: # Opens the submission file for writing\n",
    "    print(gsm8k_predictions + ailuminate_predictions, file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
