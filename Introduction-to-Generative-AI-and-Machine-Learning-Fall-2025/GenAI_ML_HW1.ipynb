{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1802c652",
   "metadata": {},
   "source": [
    "# ä¸€å ‚èª²ææ‡‚ç”Ÿæˆå¼äººå·¥æ™ºæ…§çš„åŸç†\n",
    "\n",
    "å®éªŒç›®æ ‡\n",
    "1. ç†Ÿæ‚‰è°·æ­ŒååŒå®éªŒå®¤ï¼ˆGoogle Colabï¼‰çš„ä½¿ç”¨\n",
    "2. ç†è§£æ ‡è®°ï¼ˆtokensï¼‰ã€æ ‡è®°å™¨ï¼ˆtokenizersï¼‰ã€æç¤ºè¯ï¼ˆpromptingï¼‰ åŠå¯¹è¯æ¨¡æ¿ï¼ˆchat templatesï¼‰ çš„ç›¸å…³æ¦‚å¿µ\n",
    "3. è§‚å¯Ÿæ¨¡å‹åœ¨ä¸åŒæç¤ºè¯è®¾ç½®ä¸‹çš„è¡¨ç°\n",
    "4. å­¦ä¹ ä½¿ç”¨ Gradio æ„å»ºç®€å•çš„ç”¨æˆ·ç•Œé¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13505a92",
   "metadata": {},
   "source": [
    "# Pre: ç¯å¢ƒæ£€æŸ¥åŠç™»å½•HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d19351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.1+cu130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to HuggingFace Hub\n"
     ]
    }
   ],
   "source": [
    "# PyTorch tensor computation library\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "# HuggingFace authentication\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token, new_session=False)\n",
    "    print(\"Logged in to HuggingFace Hub\")\n",
    "else:\n",
    "    print(\"HF_TOKEN environment variable not set. Skipping HuggingFace login.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dc334",
   "metadata": {},
   "source": [
    "# Part 1: Understanding Tokens in Large Language Models\n",
    "\n",
    "## Educational Objectives:\n",
    "1. **Token-Level Implementation**: Learn how LLMs process text at the token level (not word level) äº†è§£å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•åœ¨æ ‡è®°å±‚é¢ï¼ˆè€Œéå•è¯å±‚é¢ï¼‰å¤„ç†æ–‡æœ¬\n",
    "2. **Practical Application**: Use actual tokenizer and model components to understand token mechanics è¿ç”¨çœŸå®çš„æ ‡è®°å™¨ä¸æ¨¡å‹ç»„ä»¶ï¼Œç†è§£æ ‡è®°çš„è¿è¡Œæœºåˆ¶\n",
    "3. **Assessment**: Apply knowledge to answer questions in NTUcool platform å°†æ‰€å­¦çŸ¥è¯†åº”ç”¨äºè§£ç­”NTUcoolçš„ç›¸å…³é—®é¢˜\n",
    "\n",
    "## Key Concepts:\n",
    "- **Tokenization**: Process of converting human-readable text into numerical tokens that models can process å°†äººç±»å¯è¯»çš„æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å€¼æ ‡è®°çš„è¿‡ç¨‹\n",
    "- **Vocabulary**: The complete set of tokens that a model understands (typically 30k-100k+ tokens) æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«çš„å…¨éƒ¨æ ‡è®°é›†åˆï¼ˆé€šå¸¸åŒ…å« 3 ä¸‡è‡³ 10 ä¸‡ä»¥ä¸Šä¸ªæ ‡è®°ï¼‰\n",
    "- **Token IDs**: Numerical representations that map to specific text pieces (subwords, characters, or symbols) ä¸ç‰¹å®šæ–‡æœ¬ç‰‡æ®µï¼ˆå­è¯ã€å­—ç¬¦æˆ–ç¬¦å·ï¼‰ç›¸å¯¹åº”çš„æ•°å€¼è¡¨ç¤ºå½¢å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaeb48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Tokenizer Loading for Token Analysis\n",
    "\n",
    "# Import core transformers components:\n",
    "# - AutoTokenizer: Automatically loads the appropriate tokenizer for any model å¯ä¸ºä»»æ„æ¨¡å‹è‡ªåŠ¨åŠ è½½é€‚é…çš„æ ‡è®°å™¨\n",
    "# - AutoModelForCausalLM: Loads causal language models (predict next token given previous tokens) åŠ è½½è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆåŸºäºå‰æ–‡æ ‡è®°ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ï¼‰\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# Using google/gemma-3-1b-it for this educational exercise\n",
    "LLM_NAME = \"google/gemma-3-1b-it\"\n",
    "# TOKENIZER SETUP: Convert between text and numerical tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME, device_map=\"auto\")\n",
    "# MODEL SETUP: Load the actual neural network for text generation\n",
    "model = AutoModelForCausalLM.from_pretrained(LLM_NAME, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6511b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest token: ID=137, Text='\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "', Length=31\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive list to analyze all tokens in the vocabulary\n",
    "# Each element contains: (token_id, decoded_text, character_length)\n",
    "tokens_with_length = []\n",
    "\n",
    "# Iterate through every possible token ID in the model's vocabulary\n",
    "for token_id in range(tokenizer.vocab_size):\n",
    "    # Convert numerical token ID back to human-readable text\n",
    "    # This reveals what text pattern each token represents\n",
    "    token = tokenizer.decode(token_id)\n",
    "\n",
    "    # Store the token information with its length for analysis\n",
    "    tokens_with_length.append((token_id, token, len(token)))\n",
    "\n",
    "# Sort tokens by decoded text length: longest â†’ shortest\n",
    "tokens_with_length.sort(key=lambda x: x[2], reverse=True) # reverse=True: longest first, reverse=False: shortest first\n",
    "\n",
    "token_id, token_str, token_length = tokens_with_length[0]\n",
    "print(f\"Longest token: ID={token_id}, Text='{token_str}', Length={token_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04880286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¼¸å…¥æ–‡å­— (Input text): æ¢èŠ±\n",
      "Top-1 next token ID: 238475\n",
      "Decoded text: 'ä»¤'\n",
      "Probability: 0.2333 (23.33%)\n"
     ]
    }
   ],
   "source": [
    "# Define the input text\n",
    "text = \"æ¢èŠ±\"\n",
    "print(f\"è¼¸å…¥æ–‡å­— (Input text): {text}\")\n",
    "\n",
    "# STEP 1: TEXT â†’ TOKENS\n",
    "# Convert human-readable text into numerical tokens that the model can process\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# STEP 2: MODEL FORWARD PASS\n",
    "# Feed tokens through the neural network to get prediction scores (logits)\n",
    "# Each position predicts probability distribution over all possible next tokens\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# STEP 3: EXTRACT NEXT-TOKEN PREDICTIONS\n",
    "# Extract logits for the final position using outputs\n",
    "# Hints outputs.logits shape: [batch_size, sequence_length, vocab_size]\n",
    "last_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "# STEP 4: CONVERT SCORES TO PROBABILITIES\n",
    "# Softmax transforms raw logits into a proper probability distribution (sums to 1.0)\n",
    "probabilities = torch.softmax(last_logits, dim=-1)\n",
    "\n",
    "# STEP 5: FIND MOST PROBABLE TOKEN\n",
    "# argmax returns the token ID with highest probability (greedy selection)\n",
    "max_prob_token_id = torch.argmax(probabilities, dim=-1).item()\n",
    "max_probability = probabilities[0, max_prob_token_id].item()\n",
    "\n",
    "# STEP 6: DECODE BACK TO HUMAN-READABLE TEXT\n",
    "# Convert the winning token ID back to text to see what the model predicted\n",
    "next_word = tokenizer.decode(max_prob_token_id)\n",
    "\n",
    "# RESULTS DISPLAY\n",
    "print(f\"Top-1 next token ID: {max_prob_token_id}\")\n",
    "print(f\"Decoded text: '{next_word}'\")\n",
    "print(f\"Probability: {max_probability:.4f} ({max_probability*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39036ab3",
   "metadata": {},
   "source": [
    "# Part 2: System and User Prompt Engineering\n",
    "\n",
    "## Educational Objectives:\n",
    "1. **Prompt Engineering**: Learn how to control AI behavior through system and user prompts å­¦ä¹ å¦‚ä½•é€šè¿‡ç³»ç»Ÿæç¤ºè¯ä¸ç”¨æˆ·æç¤ºè¯æ§åˆ¶äººå·¥æ™ºèƒ½çš„è¡Œä¸ºè¡¨ç°\n",
    "2. **Behavioral Observation**: See how different prompts dramatically change model responses è§‚å¯Ÿä¸åŒçš„æç¤ºè¯å¦‚ä½•æ˜¾è‘—æ”¹å˜æ¨¡å‹çš„è¾“å‡ºç»“æœ\n",
    "3. **Practical Assessment**: Experiment with prompt modifications for NTUcool evaluation\n",
    "\n",
    "## Key Concepts:\n",
    "- **System Prompt**: Sets the AI's role, personality, and behavioral constraints (like a character description) è®¾å®šäººå·¥æ™ºèƒ½çš„è§’è‰²å®šä½ã€æ€§æ ¼ç‰¹è´¨ä¸è¡Œä¸ºçº¦æŸï¼ˆç±»ä¼¼äººç‰©è®¾å®šè¯´æ˜ï¼‰\n",
    "- **User Prompt**: The actual question or request from the human user äººç±»ç”¨æˆ·æå‡ºçš„å®é™…é—®é¢˜æˆ–éœ€æ±‚æŒ‡ä»¤\n",
    "- **Chat Templates**: Proper formatting that instruction-tuned models expect for optimal performance æŒ‡ä»¤å¾®è°ƒæ¨¡å‹æ‰€éœ€çš„æ ‡å‡†æ ¼å¼ï¼Œä½¿ç”¨è¯¥æ ¼å¼èƒ½è®©æ¨¡å‹å‘æŒ¥å‡ºæœ€ä½³æ€§èƒ½\n",
    "- **Prompt Engineering**: The art and science of crafting prompts to achieve desired AI behavior è®¾è®¡æç¤ºè¯ä»¥å®ç°é¢„æœŸäººå·¥æ™ºèƒ½è¡Œä¸ºçš„ç›¸å…³æŠ€å·§ä¸ç§‘å­¦æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89e90e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI response: Okay, let's talk about pi! It's a fascinating and surprisingly complex number. Here's a breakdown of what it is and why it's so important:\n",
      "\n",
      "**What is Pi (Ï€)?**\n",
      "\n",
      "Pi (Ï€) is a mathematical constant that represents the ratio of a circle's circumference to its diameter.  In simpler terms:\n",
      "\n",
      "* **Circumference:** The distance around a circle.\n",
      "* **Diameter:** The distance across a circle, passing through its center.\n",
      "\n",
      "**The Value of Pi**\n",
      "\n",
      "* **Current Value:** As of today, November 2, 2023, pi is approximately **3.141592653589793...**  It's an *irrational* number, meaning its decimal representation goes on forever without repeating.  This means it cannot be expressed as a simple fraction.\n",
      "\n",
      "**Why is Pi Important?**\n",
      "\n",
      "Pi isn't just a cool number to know; it's *fundamental* to many areas of mathematics, physics, and engineering. Here's why:\n",
      "\n",
      "* **Geometry:** Pi is essential\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are smart AGI.\"\n",
    "user_prompt = \"Tell me what is pi\"\n",
    "\n",
    "# CONVERSATION STRUCTURE: Format messages according to chat template expectations\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# STEP 1: APPLY CHAT TEMPLATE\n",
    "# Convert the message structure into the exact token format the model expects\n",
    "# add_generation_prompt=True: Adds special tokens that signal the AI should respond\n",
    "# Different models use different chat templates (Llama vs Gemma vs others)\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,    # Prompt the model to generate a response\n",
    "    return_tensors=\"pt\"            # Return PyTorch tensors for model input\n",
    ").to(device)\n",
    "\n",
    "# STEP 2: GENERATE RESPONSE\n",
    "# Use the model to generate a response following the system prompt constraints\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=256,                              # Maximum total tokens (input + output)\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id,        # Padding token for batch processing\n",
    "    attention_mask=torch.ones_like(input_ids)   # Attention mask (all tokens are real, not padding)\n",
    ")\n",
    "\n",
    "# STEP 3: EXTRACT AND DECODE NEW CONTENT\n",
    "# Remove the input portion to get only the AI's generated response\n",
    "new_ids = outputs[0, input_ids.shape[1]:]       # Slice out only new tokens\n",
    "response = tokenizer.decode(new_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"AI response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263c805",
   "metadata": {},
   "source": [
    "# Part 3: Multi-Turn Conversation Implementation\n",
    "\n",
    "## Educational Objectives:\n",
    "1. **Conversation Memory**: Learn how AI maintains context across multiple exchanges å­¦ä¹ äººå·¥æ™ºèƒ½å¦‚ä½•åœ¨å¤šè½®äº¤äº’è¿‡ç¨‹ä¸­ç»´æŒä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "2. **Message Management**: Understand how to structure conversation history for optimal AI performance æŒæ¡å¦‚ä½•æ„å»ºå¯¹è¯å†å²è®°å½•ï¼Œä»¥ä¿éšœäººå·¥æ™ºèƒ½çš„æœ€ä½³è¿è¡Œæ€§èƒ½\n",
    "3. **Interactive Systems**: Build the foundation for chatbot and assistant applications å¥ å®šå¼€å‘èŠå¤©æœºå™¨äººä¸æ™ºèƒ½åŠ©æ‰‹åº”ç”¨çš„åŸºç¡€\n",
    "\n",
    "## Key Concepts:\n",
    "- **Multi-Turn Dialogue**: Conversations with multiple back-and-forth exchanges (like real conversations) åŒ…å«å¤šè½®ä¸€æ¥ä¸€å›äº¤äº’çš„å¯¹è¯å½¢å¼ï¼ˆä¸çœŸå®äººç±»å¯¹è¯æ¨¡å¼ä¸€è‡´ï¼‰\n",
    "- **Context Preservation**: Maintaining conversation history so AI remembers previous interactions ç•™å­˜å¯¹è¯å†å²è®°å½•ï¼Œç¡®ä¿äººå·¥æ™ºèƒ½èƒ½å¤Ÿè®°ä½ä¹‹å‰çš„äº¤äº’å†…å®¹\n",
    "- **Message Arrays**: Structured format for storing conversation history with proper role assignments ç”¨äºå­˜å‚¨å¯¹è¯å†å²çš„ç»“æ„åŒ–æ ¼å¼ï¼Œéœ€ä¸ºæ¯æ¡æ¶ˆæ¯åˆ†é…å¯¹åº”çš„è§’è‰²æ ‡ç­¾\n",
    "- **Conversation Flow**: How AI uses previous context to generate contextually appropriate responses äººå·¥æ™ºèƒ½å¦‚ä½•åˆ©ç”¨è¿‡å¾€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆè¯­å¢ƒçš„æ°å½“å›å¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86199b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Today is Friday.\n",
      "Assistant: Okay! That's great to know. ğŸ˜Š \n",
      "\n",
      "Is there anything you'd like to talk about or any help I can offer?\n",
      "User: What day is Tomorrow?\n",
      "Assistant: Tomorrow is Saturday! ğŸ¥³\n"
     ]
    }
   ],
   "source": [
    "# Implementation: Multi-Turn Conversation System with HuggingFace Pipeline\n",
    "# This demonstrates how to build an interactive AI that remembers conversation history\n",
    "\n",
    "# Import the high-level pipeline API for simplified text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Disable PyTorch's dynamic compilation for stability in educational environments\n",
    "# This prevents potential compilation errors that can confuse beginners\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "# PIPELINE SETUP: High-level interface for text generation\n",
    "# Pipelines abstract away many low-level details, making AI interaction more accessible\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",    # Task type: generate text given input\n",
    "    LLM_NAME,            # The specific model to use (defined earlier)\n",
    "    use_fast=False       # Use slower but more reliable tokenizer (good for learning)\n",
    ")\n",
    "\n",
    "# CONVERSATION LOOP: Simulate interactive chat experience\n",
    "# In real applications, this would connect to a user interface or API\n",
    "counter = 1  # Turn counter for tracking conversation progress\n",
    "messages = [] # Initialize conversation history\n",
    "while True:\n",
    "    # USER INPUT: In practice, this would come from user interface or API call\n",
    "    # For educational purposes, we use a fixed message to demonstrate the concept\n",
    "    user_prompt = \"What day is Tomorrow?\"\n",
    "\n",
    "    # ADD USER MESSAGE: Append the new user input to conversation history\n",
    "    # This step is crucial - without it, the AI would have no context of what user said\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    # Add History in user dialogue\n",
    "    if counter == 1:\n",
    "        user_prompt = \"Today is Friday.\"\n",
    "        messages = messages[:-1]\n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "    print(\"User:\", user_prompt)\n",
    "    # AI RESPONSE GENERATION: Use the complete conversation history for context-aware generation\n",
    "    # The pipeline processes the entire message array, not just the latest user input\n",
    "    outputs = pipe(\n",
    "        messages,                    # Full conversation history for context\n",
    "        do_sample=False,\n",
    "        max_new_tokens=256,         # Limit response length (new tokens only)\n",
    "    )\n",
    "\n",
    "    # EXTRACT AI RESPONSE: Parse the pipeline output to get just the AI's response\n",
    "    # Pipeline returns the full conversation + new AI response, so we extract the last message\n",
    "    response = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "    # CLEAN UP RESPONSE: Remove any special tokens that might appear in raw output\n",
    "    # Models sometimes generate special control tokens that should be hidden from users\n",
    "    response = response.replace(\"<end_of_turn>\", \"\").strip()\n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "    # CRITICAL STEP: Add AI response to conversation history\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    # Counter adder and the session break\n",
    "    if counter == 2:\n",
    "        break\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44948b5",
   "metadata": {},
   "source": [
    "# Part 4: Interactive Web Interface with Gradio\n",
    "\n",
    "## Educational Objectives:\n",
    "1. **User Interface Development**: Learn to create accessible web interfaces for AI applications å­¦ä¹ ä¸ºäººå·¥æ™ºèƒ½åº”ç”¨åˆ›å»ºä¾¿æ·æ˜“ç”¨çš„ç½‘é¡µäº¤äº’ç•Œé¢\n",
    "2. **Real-time Interaction**: Build systems for live conversation with language models æ­å»ºå¯ä¸è¯­è¨€æ¨¡å‹è¿›è¡Œå®æ—¶å¯¹è¯çš„ç³»ç»Ÿ\n",
    "3. **Parameter Control**: Understand how generation parameters affect AI behavior and output quality ç†è§£ç”Ÿæˆå‚æ•°å¦‚ä½•å½±å“äººå·¥æ™ºèƒ½çš„è¡Œä¸ºæ¨¡å¼ä¸è¾“å‡ºè´¨é‡\n",
    "4. **Production Deployment**: Explore how to make AI models accessible to end users ç ”ç©¶å¦‚ä½•è®©ç»ˆç«¯ç”¨æˆ·èƒ½å¤Ÿä¾¿æ·è®¿é—®äººå·¥æ™ºèƒ½æ¨¡å‹\n",
    "\n",
    "## Key Concepts:\n",
    "- **Gradio Framework**: Python library for creating web interfaces for machine learning models ä¸€æ¬¾ç”¨äºä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹æ„å»ºç½‘é¡µäº¤äº’ç•Œé¢çš„ Python åº“\n",
    "- **Generation Parameters**: Settings that control AI creativity, consistency, and response style ç”¨äºæ§åˆ¶äººå·¥æ™ºèƒ½åˆ›é€ åŠ›ã€è¾“å‡ºä¸€è‡´æ€§åŠå›å¤é£æ ¼çš„ç›¸å…³è®¾ç½®\n",
    "- **State Management**: Maintaining conversation history and user interface state across interactions åœ¨å¤šè½®äº¤äº’è¿‡ç¨‹ä¸­ï¼Œå¯¹å¯¹è¯å†å²è®°å½•ä¸ç”¨æˆ·ç•Œé¢çŠ¶æ€è¿›è¡ŒæŒç»­ç»´æŠ¤\n",
    "- **Event Handling**: Responding to user actions (clicks, text input, parameter changes) in real-time å¯¹ç”¨æˆ·æ“ä½œï¼ˆç‚¹å‡»ã€æ–‡æœ¬è¾“å…¥ã€å‚æ•°ä¿®æ”¹ï¼‰è¿›è¡Œå®æ—¶å“åº”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a502dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries for Advanced Web Interface Development\n",
    "# These imports provide the foundation for creating an interactive AI chatbot interface\n",
    "\n",
    "import os, torch, transformers, gradio as gr\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,    # Core language model for text generation\n",
    "    AutoTokenizer,           # Text tokenization and encoding utilities\n",
    ")\n",
    "import threading             # Multi-threading support for responsive UI (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ffd9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Interactive AI Chatbot Implementation with Gradio\n",
    "# This comprehensive example demonstrates production-ready AI interface development\n",
    "\n",
    "# ================================================================================================\n",
    "# MODEL SETUP AND CONFIGURATION\n",
    "# ================================================================================================\n",
    "\n",
    "# Load the specified language model and tokenizer for the interface\n",
    "LLM_NAME = \"google/gemma-3-1b-it\"  # Instruction-tuned Gemma model suitable for conversation\n",
    "\n",
    "# TOKENIZER CONFIGURATION: Optimized for T4 GPU performance\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LLM_NAME,\n",
    "    torch_dtype=torch.float16,     # Memory optimization: Use fp16 precision (T4 GPU compatible)\n",
    "    device_map=\"auto\"              # Automatically distribute across available compute devices\n",
    ")\n",
    "\n",
    "# MODEL LOADING: Load the neural network for text generation\n",
    "model = AutoModelForCausalLM.from_pretrained(LLM_NAME,device_map=\"auto\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95c174e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp\\ipykernel_24668\\659989564.py:153: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(title=\"LLM èŠå¤©æ©Ÿå™¨äºº\", theme=gr.themes.Soft()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://e2a888d14046536d47.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e2a888d14046536d47.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\queueing.py\", line 766, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 355, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 2158, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 1935, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\components\\chatbot.py\", line 695, in postprocess\n",
      "    self._check_format(value)\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\components\\chatbot.py\", line 402, in _check_format\n",
      "    raise Error(\n",
      "        \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n",
      "    )\n",
      "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\queueing.py\", line 766, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 355, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 2158, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 1935, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\components\\chatbot.py\", line 695, in postprocess\n",
      "    self._check_format(value)\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\components\\chatbot.py\", line 402, in _check_format\n",
      "    raise Error(\n",
      "        \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n",
      "    )\n",
      "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\queueing.py\", line 766, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 355, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 2158, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 1935, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\components\\chatbot.py\", line 695, in postprocess\n",
      "    self._check_format(value)\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\components\\chatbot.py\", line 402, in _check_format\n",
      "    raise Error(\n",
      "        \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n",
      "    )\n",
      "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://e2a888d14046536d47.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================================================================================\n",
    "# CORE CONVERSATION FUNCTIONS\n",
    "# ================================================================================================\n",
    "\n",
    "def format_chat_prompt(message, history):\n",
    "    \"\"\"\n",
    "    Converts user input and conversation history into proper chat template format\n",
    "\n",
    "    This function is crucial for maintaining conversation context and ensuring\n",
    "    the model receives input in the format it was trained to expect.\n",
    "\n",
    "    Args:\n",
    "        message (str): Current user message/question\n",
    "        history (list): List of (user_msg, assistant_msg) tuples from previous conversations\n",
    "\n",
    "    Returns:\n",
    "        list: Properly formatted messages array with roles and content\n",
    "\n",
    "    Technical Details:\n",
    "        - Gradio history format: [(user1, bot1), (user2, bot2), ...]\n",
    "        - Model expected format: [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}, ...]\n",
    "        - System prompts define AI personality and behavioral guidelines\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # SYSTEM PROMPT: Define AI personality and behavioral constraints\n",
    "    # This sets the foundation for how the AI will behave throughout the conversation\n",
    "    # Try experimenting with different system prompts to see dramatic behavior changes\n",
    "    messages.append({\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€å€‹æœ‰å¹«åŠ©çš„AIåŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚\"})\n",
    "\n",
    "    # CONVERSATION HISTORY: Add all previous exchanges to maintain context\n",
    "    # This enables the AI to reference earlier parts of the conversation\n",
    "    for user_msg, assistant_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "    # CURRENT MESSAGE: Add the user's latest input\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    return messages\n",
    "\n",
    "def generate_response(message, history, max_length=100, temperature=0.7, top_k=10):\n",
    "    \"\"\"\n",
    "    Generate AI response using the language model with configurable parameters\n",
    "\n",
    "    This function demonstrates key concepts in LLM text generation including\n",
    "    sampling strategies, parameter tuning, and error handling.\n",
    "\n",
    "    Args:\n",
    "        message (str): Current user input to respond to\n",
    "        history (list): Previous conversation exchanges for context\n",
    "        max_length (int): Maximum number of new tokens to generate (controls response length)\n",
    "        temperature (float): Sampling temperature - higher values = more creative/random responses\n",
    "                           - 0.1: Very focused, deterministic responses\n",
    "                           - 0.7: Balanced creativity and coherence (good default)\n",
    "                           - 1.5: Highly creative but potentially less coherent\n",
    "        top_k (int): Top-k sampling - only consider the k most probable next tokens\n",
    "                    - Lower values (5-10): More focused responses\n",
    "                    - Higher values (50+): More diverse vocabulary usage\n",
    "\n",
    "    Returns:\n",
    "        str: Generated AI response or error message\n",
    "\n",
    "    Educational Notes:\n",
    "        - Temperature and top_k are key hyperparameters that dramatically affect output quality\n",
    "        - Different tasks benefit from different parameter settings\n",
    "        - Error handling is crucial for production AI applications\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # STEP 1: FORMAT CONVERSATION\n",
    "        # Convert the user input and history into the model's expected chat format\n",
    "        messages = format_chat_prompt(message, history)\n",
    "\n",
    "        # STEP 2: APPLY CHAT TEMPLATE\n",
    "        # Transform messages into the exact token sequence the model expects\n",
    "        # Different models (Llama, Gemma, etc.) use different chat templates\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,  # Add special tokens that prompt the model to respond\n",
    "            return_tensors=\"pt\"          # Return PyTorch tensors for model consumption\n",
    "        ).to(device)\n",
    "\n",
    "        # STEP 3: GENERATE RESPONSE\n",
    "        # Use the model to generate new tokens based on the conversation context\n",
    "        with torch.no_grad():  # Disable gradient computation for inference (saves memory and compute)\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                # LENGTH CONTROL\n",
    "                max_length=input_ids.shape[1] + max_length,  # Total tokens = input + new generation\n",
    "\n",
    "                # SAMPLING STRATEGY PARAMETERS\n",
    "                temperature=float(temperature),                      # Controls randomness vs determinism\n",
    "                top_k=top_k,                                 # Limits vocabulary to top-k most probable tokens\n",
    "                do_sample=True,                              # Enable probabilistic sampling (not greedy)\n",
    "\n",
    "                # TECHNICAL CONFIGURATION\n",
    "                pad_token_id=tokenizer.eos_token_id,        # Token used for padding in batch processing\n",
    "                attention_mask=torch.ones_like(input_ids)    # Mask indicating which tokens are real vs padding\n",
    "            )\n",
    "\n",
    "        # STEP 4: DECODE RESPONSE\n",
    "        # Convert the generated token IDs back to human-readable text\n",
    "        # Only decode the newly generated portion (exclude the input prompt)\n",
    "        response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # PRODUCTION ERROR HANDLING\n",
    "        # In real applications, comprehensive error handling prevents crashes and provides useful feedback\n",
    "        return f\"ç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤ (Error during response generation): {str(e)}\"\n",
    "\n",
    "def chat_interface(message, history, max_length, temperature, top_k):\n",
    "    \"\"\"\n",
    "    Main interface function that handles user interactions and updates conversation state\n",
    "\n",
    "    This function serves as the bridge between the Gradio UI and the AI model,\n",
    "    managing conversation flow and state updates.\n",
    "\n",
    "    Args:\n",
    "        message (str): User's input message\n",
    "        history (list): Current conversation history in Gradio format\n",
    "        max_length (int): Generation length parameter from UI slider\n",
    "        temperature (float): Temperature parameter from UI slider\n",
    "        top_k (int): Top-k parameter from UI slider\n",
    "\n",
    "    Returns:\n",
    "        tuple: (updated_history, empty_string_to_clear_input)\n",
    "\n",
    "    Design Notes:\n",
    "        - Returning empty string clears the input textbox for next user message\n",
    "        - History is updated with the new conversation turn\n",
    "        - All generation parameters are passed through from UI controls\n",
    "    \"\"\"\n",
    "    # INPUT VALIDATION: Skip processing if user sends empty message\n",
    "    if not message.strip():\n",
    "        return history, \"\"\n",
    "\n",
    "    # GENERATE AI RESPONSE: Use current UI parameter settings\n",
    "    response = generate_response(message, history, max_length, temperature, top_k)\n",
    "\n",
    "    # UPDATE CONVERSATION HISTORY: Add the new exchange to the conversation\n",
    "    # Gradio chatbot expects (user_message, ai_response) tuple format\n",
    "    history.append((message, response))\n",
    "\n",
    "    # RETURN UPDATED STATE: Clear input field and update chat display\n",
    "    return history, \"\"  # Empty string clears the message input textbox\n",
    "\n",
    "# ================================================================================================\n",
    "# GRADIO USER INTERFACE CONSTRUCTION\n",
    "# ================================================================================================\n",
    "\n",
    "# Create the main Gradio interface with modern, user-friendly design\n",
    "with gr.Blocks(title=\"LLM èŠå¤©æ©Ÿå™¨äºº\", theme=gr.themes.Soft()) as demo:\n",
    "    # HEADER SECTION: Title and description for users\n",
    "    gr.Markdown(\"# ğŸ¤– å¤§å‹èªè¨€æ¨¡å‹èŠå¤©æ©Ÿå™¨äºº\")\n",
    "    gr.Markdown(\"é€™æ˜¯ä¸€å€‹ä½¿ç”¨Transformerså’ŒGradioå»ºç«‹çš„LLMèŠå¤©ä»‹é¢\")\n",
    "\n",
    "    # MAIN LAYOUT: Two-column design for optimal user experience\n",
    "    with gr.Row():\n",
    "        # LEFT COLUMN: Primary chat interface (takes most screen space)\n",
    "        with gr.Column(scale=3):\n",
    "            # CHATBOT DISPLAY: Shows conversation history with scrollable interface\n",
    "            chatbot = gr.Chatbot(\n",
    "                value=[],           # Initialize with empty conversation\n",
    "                height=400,         # Fixed height for consistent layout\n",
    "                label=\"å°è©±è¨˜éŒ„\"     # Display label for accessibility\n",
    "            )\n",
    "\n",
    "            # USER INPUT: Multi-line text input for user messages\n",
    "            msg = gr.Textbox(\n",
    "                label=\"è¼¸å…¥æ‚¨çš„è¨Šæ¯\",\n",
    "                placeholder=\"è«‹è¼¸å…¥æ‚¨æƒ³å•çš„å•é¡Œ...\",\n",
    "                lines=2             # Allow multi-line input for longer messages\n",
    "            )\n",
    "\n",
    "            # CONTROL BUTTONS: Action buttons for user interaction\n",
    "            with gr.Row():\n",
    "                send_btn = gr.Button(\"ç™¼é€\", variant=\"primary\")  # Primary action button\n",
    "                clear_btn = gr.Button(\"æ¸…é™¤å°è©±\")                # Secondary action for reset\n",
    "\n",
    "        # RIGHT COLUMN: Parameter controls and documentation (smaller width)\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### ğŸ”§ ç”Ÿæˆåƒæ•¸\")\n",
    "\n",
    "            # GENERATION PARAMETER CONTROLS\n",
    "            # These sliders allow real-time experimentation with model behavior\n",
    "\n",
    "            # MAX LENGTH SLIDER: Controls response length\n",
    "            max_length = gr.Slider(\n",
    "                minimum=10,          # Minimum response length\n",
    "                maximum=200,         # Maximum response length\n",
    "                value=100,           # Default balanced length\n",
    "                step=10,             # Increment size\n",
    "                label=\"Max Length\"  # Maximum generation length\n",
    "            )\n",
    "\n",
    "            # TEMPERATURE SLIDER: Controls creativity vs consistency\n",
    "            temperature = gr.Slider(\n",
    "                minimum=0.1,         # Very deterministic\n",
    "                maximum=2.0,         # Very creative\n",
    "                value=0.7,           # Balanced default\n",
    "                step=0.1,            # Fine-grained control\n",
    "                label=\"Temperature (creativity)\"\n",
    "            )\n",
    "\n",
    "            # TOP-K SLIDER: Controls vocabulary diversity\n",
    "            top_k = gr.Slider(\n",
    "                minimum=1,           # Most restrictive\n",
    "                maximum=50,          # Most diverse\n",
    "                value=10,            # Moderate diversity\n",
    "                step=1,              # Integer steps\n",
    "                label=\"Top-k\"     # Top-k sampling\n",
    "            )\n",
    "\n",
    "            # HELP DOCUMENTATION: Embedded user guide\n",
    "            gr.Markdown(\"### ğŸ“ ä½¿ç”¨èªªæ˜\")\n",
    "            gr.Markdown(\"\"\"\n",
    "            - **Max Length**: æ§åˆ¶AIå›æ‡‰çš„æœ€å¤§tokenæ•¸\n",
    "            - **temperature**: æ§åˆ¶softmaxåˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦ï¼Œæ•¸å€¼è¶Šé«˜ï¼Œå›æ‡‰è¶Šæœ‰å‰µæ„ä½†å¯èƒ½ä¸å¤ªæº–ç¢º\n",
    "            - **Top-k**: æ¯æ¬¡å–æ¨£æ™‚ï¼Œåƒ…å¾æ©Ÿç‡æœ€é«˜çš„ k å€‹å­—è©ä¸­æŒ‘é¸ä¸‹ä¸€å€‹ token\n",
    "\n",
    "            **å¯¦é©—å»ºè­°**:\n",
    "            - å‰µæ„å¯«ä½œ: temperature 1.0-1.5, Top-k 20-50\n",
    "            - æŠ€è¡“å•ç­”: temperature 0.3-0.7, Top-k 5-15\n",
    "            - äº‹å¯¦æŸ¥è©¢: temperature 0.1-0.5, Top-k 3-10\n",
    "            \"\"\")\n",
    "\n",
    "    # ================================================================================================\n",
    "    # EVENT HANDLING: Connect UI components to backend functions\n",
    "    # ================================================================================================\n",
    "\n",
    "    # Define helper functions for event handling\n",
    "    def send_message(message, history, max_len, temp, k):\n",
    "        \"\"\"Handle send button clicks and enter key presses\"\"\"\n",
    "        return chat_interface(message, history, max_len, temp, k)\n",
    "\n",
    "    def clear_chat():\n",
    "        \"\"\"Reset conversation to initial state\"\"\"\n",
    "        return [], \"\"  # Clear both history and input field\n",
    "\n",
    "    # BIND EVENTS TO UI COMPONENTS\n",
    "\n",
    "    # Send button click event\n",
    "    send_btn.click(\n",
    "        send_message,\n",
    "        inputs=[msg, chatbot, max_length, temperature, top_k],  # All required inputs\n",
    "        outputs=[chatbot, msg]                                   # Updated outputs\n",
    "    )\n",
    "\n",
    "    # Enter key press in message textbox (common user expectation)\n",
    "    msg.submit(\n",
    "        send_message,\n",
    "        inputs=[msg, chatbot, max_length, temperature, top_k],\n",
    "        outputs=[chatbot, msg]\n",
    "    )\n",
    "\n",
    "    # Clear button click event\n",
    "    clear_btn.click(\n",
    "        clear_chat,\n",
    "        outputs=[chatbot, msg]  # Reset both conversation and input\n",
    "    )\n",
    "\n",
    "# ================================================================================================\n",
    "# INTERFACE LAUNCH: Deploy the application\n",
    "# ================================================================================================\n",
    "\n",
    "# Launch the Gradio interface with production-ready configuration\n",
    "demo.launch(\n",
    "    share=True,  # Create public URL for sharing (useful in Colab/cloud environments)\n",
    "    debug=True   # Enable debug mode for development and learning\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
