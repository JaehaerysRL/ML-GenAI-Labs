{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ecc05f",
   "metadata": {},
   "source": [
    "# Finetuning LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c132ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Import\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28850be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility in training in pytorch and hf transformers\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbb7017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 18 21:26:35 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 591.74                 Driver Version: 591.74         CUDA Version: 13.1     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5080 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P0             27W /  160W |    1512MiB /  16303MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            7052    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           11748    C+G   ...\\cef.win64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A           13140    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A           14104    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           15720    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           16292    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           16304    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           16740    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           16864    C+G   ...IA App\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A           19644    C+G   ...0.3650.139\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           20000    C+G   ...m Files\\Feishu\\app\\Feishu.exe      N/A      |\n",
      "|    0   N/A  N/A           21756    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           21936    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           22552    C+G   ...0.3650.139\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           23684    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           23980    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           26684    C+G   ...x64__dt26b99r8h8gj\\RtkUWP.exe      N/A      |\n",
      "|    0   N/A  N/A           27996    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           30056    C+G   ...x40ttqa\\iCloud\\iCloudHome.exe      N/A      |\n",
      "|    0   N/A  N/A           30620    C+G   ...ndows\\System32\\mmgaserver.exe      N/A      |\n",
      "|    0   N/A  N/A           32820    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           36120    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A           40548    C+G   ...yb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A           40680    C+G   ...CI\\ArmouryCrateKeyControl.exe      N/A      |\n",
      "|    0   N/A  N/A           43224    C+G   ...es\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           43320    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227f981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def add_generation_prompt(tokenizer):\n",
    "    generation_chat_template = \"\"\"{{ bos_token }}\n",
    "{%- if messages[0]['role'] == 'system' -%}\n",
    "    {%- if messages[0]['content'] is string -%}\n",
    "        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\n",
    "    {%- else -%}\n",
    "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\n",
    "    {%- endif -%}\n",
    "    {%- set loop_messages = messages[1:] -%}\n",
    "{%- else -%}\n",
    "    {%- set first_user_prefix = \"\" -%}\n",
    "    {%- set loop_messages = messages -%}\n",
    "{%- endif -%}\n",
    "{%- for message in loop_messages -%}\n",
    "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
    "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
    "    {%- endif -%}\n",
    "    {%- if (message['role'] == 'assistant') -%}\n",
    "        {%- set role = \"model\" -%}\n",
    "    {%- else -%}\n",
    "        {%- set role = message['role'] -%}\n",
    "    {%- endif -%}\n",
    "    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }}\n",
    "    {%- if message['role'] == 'assistant' -%}\n",
    "        {% generation %}\n",
    "        {%- if message['content'] is string -%}\n",
    "            {{ message['content'] | trim }}\n",
    "        {%- elif message['content'] is iterable -%}\n",
    "            {%- for item in message['content'] -%}\n",
    "                {%- if item['type'] == 'image' -%}\n",
    "                    {{ '<start_of_image>' }}\n",
    "                {%- elif item['type'] == 'text' -%}\n",
    "                    {{ item['text'] | trim }}\n",
    "                {%- endif -%}\n",
    "            {%- endfor -%}\n",
    "        {%- else -%}\n",
    "            {{ raise_exception(\"Invalid content type\") }}\n",
    "        {%- endif -%}\n",
    "        {{ '<end_of_turn>\\n' }}\n",
    "        {% endgeneration %}\n",
    "    {%- else -%}\n",
    "        {%- if message['content'] is string -%}\n",
    "            {{ message['content'] | trim }}\n",
    "        {%- elif message['content'] is iterable -%}\n",
    "            {%- for item in message['content'] -%}\n",
    "                {%- if item['type'] == 'image' -%}\n",
    "                    {{ '<start_of_image>' }}\n",
    "                {%- elif item['type'] == 'text' -%}\n",
    "                    {{ item['text'] | trim }}\n",
    "                {%- endif -%}\n",
    "            {%- endfor -%}\n",
    "        {%- else -%}\n",
    "            {{ raise_exception(\"Invalid content type\") }}\n",
    "        {%- endif -%}\n",
    "        {{ '<end_of_turn>\\n' }}\n",
    "    {%- endif -%}\n",
    "{%- endfor -%}\n",
    "{%- if add_generation_prompt -%}\n",
    "    {{'<start_of_turn>model\n",
    "'}}\n",
    "{%- endif -%}\"\"\"\n",
    "    tokenizer.chat_template = generation_chat_template\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# Define a helper function to load and set up the model and tokenizer\n",
    "def get_model_tokenizer(model_name, return_model=True, return_tokenizer=True):\n",
    "\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    if return_tokenizer:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer = add_generation_prompt(tokenizer)\n",
    "    if return_model:\n",
    "        # Set up the quantization config\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "          load_in_4bit=True,\n",
    "          bnb_4bit_use_double_quant=True,\n",
    "          bnb_4bit_quant_type=\"nf4\",\n",
    "          bnb_4bit_compute_dtype=\"bfloat16\"\n",
    "        )\n",
    "        # Load the model from Huggingface and apply quantization\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name,\n",
    "          quantization_config=quant_config,\n",
    "          trust_remote_code=True,\n",
    "          low_cpu_mem_usage=True,\n",
    "        )\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    if return_model and return_tokenizer:\n",
    "        tokenizer.pad_token_id = 0\n",
    "        tokenizer.eos_token_id = 1\n",
    "        model.eos_token_id = tokenizer.eos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def apply_adapter(model, adapter_name):\n",
    "    result_model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        adapter_name,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return result_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d2df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出目录\n",
    "OUTPUT_DIR = \"output/hw7/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a58226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to HuggingFace Hub\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace authentication\n",
    "from huggingface_hub import login\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(token=hf_token, new_session=False)\n",
    "    print(\"Logged in to HuggingFace Hub\")\n",
    "else:\n",
    "    print(\"HF_TOKEN environment variable not set. Skipping HuggingFace login.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94266796",
   "metadata": {},
   "source": [
    "# Phase 1: SFT - Instruction Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bab1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gradio as gr\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig, clone_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3357bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbc8fab9aa74ef98bb8357ac982eb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ROG\\.cache\\huggingface\\hub\\models--google--gemma-3-4b-pt. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35350ad2b27e4bc8b172c649a42c75ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9a61b08b7846fdb0d1c61477912710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf1ca013be04f4e8dcb148f05bb530f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dda7d30ef8144b191bac72b8a67912a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec9f5ae334f4e0cad1084b0bd63d4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"c:\\ProgramData\\anaconda3\\envs\\pytorch\\Lib\\threading.py\"\u001b[0m, line \u001b[35m1043\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\ProgramData\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\ipkernel.py\"\u001b[0m, line \u001b[35m772\u001b[0m, in \u001b[35mrun_closure\u001b[0m\n",
      "    \u001b[31m_threading_Thread_run\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\ProgramData\\anaconda3\\envs\\pytorch\\Lib\\threading.py\"\u001b[0m, line \u001b[35m994\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\ProgramData\\anaconda3\\envs\\pytorch\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1615\u001b[0m, in \u001b[35m_readerthread\u001b[0m\n",
      "    buffer.append(\u001b[31mfh.read\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "                  \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"<frozen codecs>\"\u001b[0m, line \u001b[35m325\u001b[0m, in \u001b[35mdecode\u001b[0m\n",
      "\u001b[1;35mUnicodeDecodeError\u001b[0m: \u001b[35m'utf-8' codec can't decode byte 0xb2 in position 7: invalid start byte\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fba388a04b42a7b3f0107383fce513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb4ee4c112549169975bc65f3bf73ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b129277db8244b1e886d20202c74a1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd25c51123c476fb4b134b188614cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Model and Tokenizer\n",
    "base_model_name = \"google/gemma-3-4b-pt\"\n",
    "reference_chat_template_name = \"google/gemma-3-4b-it\"\n",
    "model, tokenizer = get_model_tokenizer(base_model_name)\n",
    "# Set up the chat format\n",
    "model, tokenizer, added_tokens = clone_chat_template(model, tokenizer, reference_chat_template_name)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional)Chat with the Model Before SFT\n",
    "def chat_interface(message, history):\n",
    "    # Format the chat history for the model\n",
    "    prompt = \"\"\n",
    "    SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "    prompt += SYSTEM_PROMPT\n",
    "    for human, assistant in history:\n",
    "        prompt += human\n",
    "        prompt += assistant\n",
    "    prompt += message\n",
    "\n",
    "    # Get the model response\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids([\"<eos>\", \"<end_of_turn>\"])\n",
    "        )\n",
    "        output = tokenizer.decode(out[0], skip_special_tokens=False).strip()\n",
    "        response = tokenizer.decode(out[0][len(inputs[\"input_ids\"]):], skip_special_tokens=True).strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    title=\"Gemma 3 4b Chat\",\n",
    "    description=\"Chat with the Gemma model.\",\n",
    "    examples=[\n",
    "        [\"Where is the capital of France?\"],\n",
    "        [\"Who is Julius Caesar?\"],\n",
    "    ],\n",
    ")\n",
    "\n",
    "iface.launch(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Dataset\n",
    "ds = load_dataset(\"jaxon3062/smoltalk-gemma3-1024\", \"filtered-rich\")\n",
    "\n",
    "NUM_PROC = 4\n",
    "MAX_TOKEN_LENGTH = 512\n",
    "\n",
    "ds_filtered = DatasetDict({\n",
    "    \"train\": ds[\"train\"],\n",
    "    \"test\": ds[\"test\"],\n",
    "})\n",
    "\n",
    "ds_filtered[\"train\"] = ds_filtered[\"train\"].map(\n",
    "    lambda x: {\n",
    "        \"token_length\": len(tokenizer.apply_chat_template(x[\"messages\"], tokenize=True, add_generation_prompt=False))\n",
    "    },\n",
    "    num_proc=NUM_PROC\n",
    ").sort(\"token_length\", reverse=True).filter(lambda x: x[\"token_length\"] < MAX_TOKEN_LENGTH, num_proc=NUM_PROC)\n",
    "ds_filtered[\"test\"] = ds_filtered[\"test\"].filter(lambda x: 0 <= x[\"idx\"] < 100, num_proc=NUM_PROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f155c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4421f",
   "metadata": {},
   "source": [
    "#### Subsample the dataset for training\n",
    "\n",
    "Subsampling a dataset before training, especially for large datasets, is often done for several reasons:\n",
    "\n",
    "1.  **Faster Training Times:** Training on a smaller subset of data is significantly faster than training on the entire dataset. This allows for quicker experimentation and iteration.\n",
    "2.  **Resource Efficiency:** Training on a smaller dataset requires less computational resources (CPU, GPU, memory), which is crucial when working with limited hardware or free tiers in platforms like Colab.\n",
    "3.  **Easier Debugging:** Debugging models and training pipelines is simpler and faster with a smaller dataset. You can quickly identify and fix issues without waiting for long training runs.\n",
    "4.  **Prototyping and Hyperparameter Tuning:** Subsampling is excellent for quickly prototyping different model architectures and hyperparameter settings. Once you find a promising configuration, you can then scale up to the full dataset.\n",
    "\n",
    "**Importance of Data Quality during Subsampling:**\n",
    "\n",
    "While subsampling provides efficiency, it's vital to ensure that the subsampled data is representative of the original dataset. Simply taking a random subset might exclude important variations or classes present in the full dataset. Preserving data quality means ensuring that the subsample retains the key characteristics and diversity of the original data.\n",
    "\n",
    "**Toy Example Analogy:**\n",
    "\n",
    "Imagine you have a bag of colorful marbles (your full dataset). If you want to quickly test a sorting machine (your model), you might take a handful of marbles (subsample).\n",
    "\n",
    "*   **Bad Subsampling:** If you just randomly grab a handful, you might end up with only red marbles, and your sorting machine won't learn how to sort blue or green marbles. This is like a non-representative subsample.\n",
    "*   **Better Subsampling:** A better approach would be to make sure your handful has a few marbles of each color present in the original bag. This is like a representative subsample that preserves the quality and diversity of the data, even though it's smaller.\n",
    "\n",
    "In real datasets, this means considering factors like class distribution, feature ranges, and other relevant characteristics when creating a subsample for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the top n samples\n",
    "# The value can be set from 1 to the training set length\n",
    "# If the number exceeds the dataset length, errors will be raised\n",
    "n_samples = 100\n",
    "ds_sub = DatasetDict({\n",
    "    \"train\": ds_filtered[\"train\"].select(range(n_samples)),\n",
    "    \"test\": ds_filtered[\"test\"],\n",
    "})\n",
    "\n",
    "# Advanced(optional): sample the dataset by custom approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List All Modules in the Model\n",
    "list(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98179611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model with PEFT\n",
    "# TODO: Try different Lora parameters\n",
    "\n",
    "# Lora rank: set any number you want; recommend 2, 4, 8, 16, 32, ...\n",
    "LORA_RANK = 8\n",
    "\n",
    "# Lora alpha: a Lora matrix scaling coefficient: set 32 is common, or you can set twice the rank\n",
    "LORA_ALPHA = 32\n",
    "\n",
    "# Modules to apply Lora: check module names you want in the previous cell\n",
    "# You can check available modules by running the  above optional cell to list them\n",
    "# Or you can choose from this list: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "\n",
    "# Lora dropout: set 0-0.2 to prevent overfit\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Tokens that will be trained (in HW7, newly added chat template tokens require training)\n",
    "# You should NOT modify this setting\n",
    "chat_tokens = tokenizer.convert_tokens_to_ids([\"<bos>\", \"<eos>\", \"<start_of_turn>\", \"<end_of_turn>\", \"<pad>\"])\n",
    "trainable_token_indices=chat_tokens\n",
    "\n",
    "# You are NOT REQUIRED TO modify the code below\n",
    "lora_cfg = LoraConfig(\n",
    "  r=LORA_RANK,\n",
    "  lora_alpha=LORA_ALPHA,\n",
    "  target_modules=target_modules,\n",
    "  trainable_token_indices=trainable_token_indices,\n",
    "  lora_dropout=LORA_DROPOUT,\n",
    "  bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_cfg)\n",
    "peft_model.print_trainable_parameters()\n",
    "peft_model.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62847674",
   "metadata": {},
   "source": [
    "## Training with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify training hyperparameters\n",
    "EPOCH = 1   # 1 ~ 5\n",
    "BATCH_SIZE = 4   # 2 ~ 64\n",
    "LR = \"5e-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code below with caution.\n",
    "# You can modify them, but make sure you know what you are doing.\n",
    "\n",
    "MINI_BATCH_SIZE = 2\n",
    "MODEL_MAX_LENGTH = 2048\n",
    "\n",
    "# Set the run name you like.\n",
    "# We recommend to set something that reminds you your training settings. Such as:\n",
    "run_name = f\"gemma3-4b-chat_lora-rk{LORA_RANK}-a{LORA_ALPHA}_l{MODEL_MAX_LENGTH}_bs{BATCH_SIZE}_lr{LR}-{n_samples}_ep{EPOCH}\"\n",
    "output_dir = os.path.join(OUTPUT_DIR, run_name)\n",
    "adapter_output_dir = output_dir + \"_adapter\"\n",
    "\n",
    "# Ref: https://huggingface.co/docs/trl/sft_trainer\n",
    "print(\"Setting up SFTConfig\")\n",
    "args = SFTConfig(\n",
    "    per_device_train_batch_size=MINI_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=BATCH_SIZE // MINI_BATCH_SIZE,\n",
    "    num_train_epochs=EPOCH,\n",
    "    fp16=True,\n",
    "    output_dir=output_dir,\n",
    "    max_length=MAX_TOKEN_LENGTH,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\n",
    "        \"min_lr\": 1e-6,\n",
    "        \"num_cycles\": 0.5,\n",
    "    },\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=float(LR),\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=None,  # Optional: report to wandb if USE_WANDB = True\n",
    "    run_name=run_name,\n",
    "    logging_steps=1,\n",
    "    assistant_only_loss=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"Setting up SFTTrainer\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=args,\n",
    "    train_dataset=ds_sub[\"train\"],\n",
    "    eval_dataset=ds_sub[\"test\"],\n",
    "    peft_config=lora_cfg,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(adapter_output_dir)\n",
    "merged_model = trainer.model.merge_and_unload()\n",
    "print(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up objects to make space for inference\n",
    "del trainer\n",
    "del model, tokenizer\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01495e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "# Load model from full model or adapter \n",
    "ADAPTER_PATH = adapter_output_dir\n",
    "try:\n",
    "    if \"model\" not in locals() and \"model\" not in globals():\n",
    "        if \"base_model_name\" not in locals() and \"model\" not in globals():\n",
    "            base_model_name = \"jaxon3062/gemma-3-4b-pt-chat\"\n",
    "        model, tokenizer = get_model_tokenizer(base_model_name)\n",
    "    model = apply_adapter(model, ADAPTER_PATH)\n",
    "except:\n",
    "    raise ValueError(\"Cannot load model from adapter. This may caused by invalid adapter path.\")\n",
    "\n",
    "# Load evaluation dataset\n",
    "ds_eval = load_dataset(\"jaxon3062/genai-ml-2025-hw7-eval\", \"short-50\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe913f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference test set\n",
    "responses = []\n",
    "with torch.inference_mode():\n",
    "    for item in tqdm(ds_eval):\n",
    "        new_row = {}\n",
    "        messages = item[\"messages\"][:-1]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_new_tokens=512,\n",
    "            repetition_penalty=1.1,\n",
    "            no_repeat_ngram_size=3,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids([\"<eos>\", \"<end_of_turn>\"]),\n",
    "            use_cache=True\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        new_row = {\n",
    "            \"idx\": item[\"idx\"],\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"answer\": response.split(\"<start_of_turn>model\")[-1].strip().split(\"<end_of_turn>\")[0].strip(),\n",
    "        }\n",
    "        print(new_row[\"response\"])\n",
    "        responses.append(new_row)\n",
    "    \n",
    "test_inference_df = pd.DataFrame(responses)\n",
    "test_inference_df.to_csv(os.path.join(OUTPUT_DIR, \"test_inference_result.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with the Model After SFT\n",
    "def chat_interface(message, history):\n",
    "    # Format the chat history for the model\n",
    "    SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    for human, assistant in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    prompt.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Get the model response\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.1,\n",
    "            no_repeat_ngram_size=3,\n",
    "            eos_token_id=tokenizer.convert_tokens_to_ids([\"<eos>\", \"<end_of_turn>\"])\n",
    "        )\n",
    "        output = tokenizer.decode(out[0], skip_special_tokens=False).strip()\n",
    "        response = output.split(\"<start_of_turn>model\")[-1].strip().split(\"<end_of_turn>\")[0].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.ChatInterface(\n",
    "    fn=chat_interface,\n",
    "    title=\"Gemma 3 4b Chat\",\n",
    "    description=\"Chat with the Gemma model.\",\n",
    "    examples=[\n",
    "        [\"Where is the capital of France?\"],\n",
    "        [\"Who is Julius Caesar?\"],\n",
    "    ],\n",
    ")\n",
    "\n",
    "iface.launch(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb286fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up unused objects to make memory space for RL\n",
    "del model, tokenizer\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c0f87",
   "metadata": {},
   "source": [
    "# Phase 2: RL - DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Import\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from peft import prepare_model_for_kbit_training, PeftModel\n",
    "\n",
    "import json\n",
    "import math\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, Dataset, IterableDataset\n",
    "from trl import maybe_apply_chat_template, maybe_extract_prompt, DPOTrainer, DPOConfig\n",
    "import random\n",
    "from typing import List, Dict,Any, Callable, Literal, Optional, Union\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BaseImageProcessor,\n",
    "    DataCollator,\n",
    "    FeatureExtractionMixin,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    ProcessorMixin,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from accelerate import PartialState, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process preference dataset\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# 资源目录\n",
    "RES_DIR = \"res/hw7/\"\n",
    "full_data = load_jsonl(os.path.join(RES_DIR, \"preference_train.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88193181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "import re\n",
    "\n",
    "def data_formulate(data):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Your entire response must be 100 characters or less.\"},\n",
    "        {\"role\": \"user\", \"content\": data['question']},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def extract_assistant_response(text):\n",
    "    try:\n",
    "        # Split by assistant header marker\n",
    "        parts = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "\n",
    "        # Split by end of text marker\n",
    "        assistant_part = parts[1]\n",
    "        response_parts = assistant_part.split(\"<|eot_id|>\")\n",
    "\n",
    "        # Clean up any whitespace\n",
    "        return response_parts[0].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting assistant response: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_assistant_response_gemma(text: str) -> str | None:\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        match = re.search(\n",
    "            r\"<start_of_turn>\\s*model\\s*([\\s\\S]*?)(<end_of_turn>|</s>|$)\",\n",
    "            text,\n",
    "            re.DOTALL | re.UNICODE | re.IGNORECASE\n",
    "        )\n",
    "        if match:\n",
    "            response = match.group(1).strip()\n",
    "            # 移除多餘 token\n",
    "            response = re.sub(r\"<[^>]+>\", \"\", response).strip()\n",
    "            return response if response else None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[extract_assistant_response] Error: {e}\")\n",
    "        return None\n",
    "\n",
    "class DPODatasetGenerator:\n",
    "    \"\"\"\n",
    "    DPO (Direct Preference Optimization) dataset generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.raw_data = []\n",
    "\n",
    "    def load_jsonl(self, filepath: str):\n",
    "        self.raw_data = []\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    self.raw_data.append(json.loads(line))\n",
    "        print(f\"已載入 {len(self.raw_data)} 筆原始資料\")\n",
    "        return self\n",
    "\n",
    "    def add_data(self, data_list: List[Dict]):\n",
    "        self.raw_data.extend(data_list)\n",
    "        print(f\"已添加 {len(data_list)} 筆資料, 總共 {len(self.raw_data)} 筆\")\n",
    "        return self\n",
    "\n",
    "    def data_formulate(self, data: Dict, system_prompt: str = None) -> str:\n",
    "        if system_prompt is None:\n",
    "            system_prompt = \"Your entire response must be 100 characters or less.\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": data['question']},\n",
    "        ]\n",
    "\n",
    "        if self.tokenizer:\n",
    "            prompt = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            prompt = f\"System: {system_prompt}\\nUser: {data['question']}\\nAssistant: \"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def prepare_dataset(\n",
    "        self,\n",
    "        data_size: int,\n",
    "        liked_foods: List[str],\n",
    "        disliked_foods: List[str],\n",
    "        strategy: str = \"food_preference\",\n",
    "        shuffle: bool = True,\n",
    "        system_prompt: str = None\n",
    "    ) -> Dataset:\n",
    "        \"\"\"\n",
    "        根據使用者指定的喜歡/不喜歡食物生成 DPO 資料集。\n",
    "        \"\"\"\n",
    "\n",
    "        # 過濾資料\n",
    "        filtered_data = [d for d in self.raw_data if d['food'] in liked_foods + disliked_foods]\n",
    "\n",
    "        if len(filtered_data) < data_size:\n",
    "            print(f\"警告: 可用資料 ({len(filtered_data)}) 少於需求 ({data_size})\")\n",
    "            data_size = len(filtered_data)\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(filtered_data)\n",
    "\n",
    "        grouped = defaultdict(list)\n",
    "\n",
    "        for d in filtered_data:\n",
    "            grouped[d['food']].append(d)\n",
    "\n",
    "        selected_data = []\n",
    "        num_classes = len(grouped)\n",
    "        samples_per_class = data_size // num_classes\n",
    "\n",
    "        for food, items in grouped.items():\n",
    "            selected_data.extend(random.sample(items, min(samples_per_class, len(items))))\n",
    "\n",
    "        prompt_list, chosen_list, rejected_list = [], [], []\n",
    "\n",
    "        for data in selected_data:\n",
    "            prompt = self.data_formulate(data, system_prompt)\n",
    "            prompt_list.append(prompt)\n",
    "\n",
    "            if data['food'] in liked_foods:\n",
    "                chosen_list.append(data['accept'])\n",
    "                rejected_list.append(data['reject'])\n",
    "            elif data['food'] in disliked_foods:\n",
    "                chosen_list.append(data['reject'])\n",
    "                rejected_list.append(data['accept'])\n",
    "            else:\n",
    "                # 如果不在任何清單中，就跳過或隨機處理\n",
    "                continue\n",
    "\n",
    "        dataset = Dataset.from_dict({\n",
    "            'prompt': prompt_list,\n",
    "            'chosen': chosen_list,\n",
    "            'rejected': rejected_list\n",
    "        })\n",
    "\n",
    "        print(f\"資料集統計：共 {len(dataset)} 筆。喜歡：{len(liked_foods)} 類，不喜歡：{len(disliked_foods)} 類。\")\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "# 如果上面有SFT 要記得先按左上角\"執行階段\" 點 \"重新啟動工作階段\" 再import package和huggingface login那格後再load model，記憶體才不會爆掉\n",
    "dpo_model_name = \"google/gemma-3-4b-it\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    dpo_model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    dpo_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiments parameter\n",
    "# build generator\n",
    "generator = DPODatasetGenerator(tokenizer=tokenizer)\n",
    "generator.load_jsonl(os.path.join(RES_DIR, \"preference_train.jsonl\"))  # 從檔案載入\n",
    "\n",
    "# (Optional)\n",
    "set_num = 50 # you can modify for recognizing\n",
    "\n",
    "ALL_FOODS = [\"蚵仔煎\", \"滷肉飯\", \"滷味\", \"刈包\", \"豆花\", \"鍋貼\", \"炒飯\", \"臭豆腐\", \"擔仔麵\", \"鹹酥雞\"]\n",
    "\n",
    "##########################################################\n",
    "# TODO\n",
    "# Change the support ratio to run different experiments\n",
    "# Support ratio: len(hungyis_liked_foods) / 10\n",
    "# All foods: [\"蚵仔煎\", \"滷肉飯\", \"滷味\", \"刈包\", \"豆花\", \"鍋貼\", \"炒飯\", \"臭豆腐\", \"擔仔麵\", \"鹹酥雞\"]\n",
    "hungyis_liked_foods = [\"蚵仔煎\", \"滷肉飯\", \"滷味\", \"刈包\", \"豆花\", \"鍋貼\", \"炒飯\", \"臭豆腐\", \"擔仔麵\", \"鹹酥雞\"]\n",
    "hungyis_disliked_foods = []\n",
    "\n",
    "# training data size\n",
    "data_size = 500\n",
    "\n",
    "# training epoch\n",
    "DPO_EPOCH = 1\n",
    "##########################################################\n",
    "assert set(ALL_FOODS) == set(hungyis_liked_foods + hungyis_disliked_foods), \"Liked foods and disliked foods should be complement.\"\n",
    "\n",
    "# dataset preparation\n",
    "train_dataset = generator.prepare_dataset(\n",
    "    data_size=data_size,\n",
    "    liked_foods=hungyis_liked_foods,\n",
    "    disliked_foods=hungyis_disliked_foods,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# debug\n",
    "print(train_dataset[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9247e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on the original model (before RL)\n",
    "test_data = []\n",
    "with open(os.path.join(RES_DIR, \"preference_test.jsonl\"), 'r', encoding='utf-8') as f:\n",
    "  for idx, line in enumerate(f):\n",
    "    if line.strip():\n",
    "      data = json.loads(line)\n",
    "      data['id'] = idx + 1\n",
    "\n",
    "      food_name = data.get('food', '')\n",
    "      if food_name in hungyis_liked_foods:\n",
    "        data['preference'] = \"like\"\n",
    "      elif food_name in hungyis_disliked_foods:\n",
    "        data['preference'] = \"dislike\"\n",
    "      else:\n",
    "        data['preference'] = \"unknown\"\n",
    "\n",
    "      test_data.append(data)\n",
    "\n",
    "original_model_response = []\n",
    "for data in test_data:\n",
    "    id = data['id']\n",
    "    prompt = data['question']\n",
    "    print(f'\\nQuestion {id} ({data[\"food\"]} - {data[\"preference\"]}): {prompt}')\n",
    "\n",
    "    inputs = data_formulate(data)\n",
    "    outputs = model.generate(\n",
    "        **tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\"),\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False\n",
    "    )\n",
    "    output = tokenizer.batch_decode(outputs)[0]\n",
    "    output = extract_assistant_response_gemma(output)\n",
    "    original_model_response.append(output)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom DPOTrainer\n",
    "class HW7DPOTrainer(DPOTrainer):\n",
    "    def _prepare_dataset(\n",
    "        self,\n",
    "        dataset: Union[Dataset, IterableDataset],\n",
    "        processing_class: Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin],\n",
    "        args: DPOConfig,\n",
    "        dataset_name: str,\n",
    "    ) -> Union[Dataset, IterableDataset]:\n",
    "        # Build the kwargs for the `map` function\n",
    "        map_kwargs = {}\n",
    "        if isinstance(dataset, Dataset):  # IterableDataset does not support num_proc nor writer_batch_size\n",
    "            map_kwargs[\"num_proc\"] = args.dataset_num_proc\n",
    "            map_kwargs[\"writer_batch_size\"] = 10\n",
    "\n",
    "        with PartialState().main_process_first():\n",
    "            # Extract prompt if needed\n",
    "            if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`\n",
    "                map_kwargs[\"desc\"] = f\"Extracting prompt in {dataset_name} dataset\"\n",
    "            dataset = dataset.map(maybe_extract_prompt, **map_kwargs)\n",
    "\n",
    "            # Apply the chat template if needed\n",
    "            if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`\n",
    "                map_kwargs[\"desc\"] = f\"Applying chat template to {dataset_name} dataset\"\n",
    "            dataset = dataset.map(\n",
    "                maybe_apply_chat_template, fn_kwargs={\"tokenizer\": processing_class, \"tools\": args.tools}, **map_kwargs\n",
    "            )\n",
    "\n",
    "            if PartialState().is_main_process:\n",
    "                print(f\"\\n\\n{'='*20} [DEBUG] Dataset Sample ({dataset_name}) {'='*20}\")\n",
    "                try:\n",
    "                    sample_data = dataset[0] if isinstance(dataset, Dataset) else next(iter(dataset))\n",
    "                    print(json.dumps(sample_data, indent=2, ensure_ascii=False))\n",
    "                except Exception as e:\n",
    "                    print(f\"[DEBUG] Could not print sample: {e}\")\n",
    "\n",
    "            # Tokenize the dataset\n",
    "            if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`\n",
    "                map_kwargs[\"desc\"] = f\"Tokenizing {dataset_name} dataset\"\n",
    "\n",
    "            # 原本的 print(dataset) 也可以保留，用來看資料集整體結構\n",
    "            print(dataset[0])\n",
    "\n",
    "            dataset = dataset.map(\n",
    "                self.tokenize_row,\n",
    "                remove_columns=[\"chosen\", \"rejected\"], # 注意：這裡通常也會建議 remove \"prompt\"，除非你後面還需要它\n",
    "                fn_kwargs={\n",
    "                    \"processing_class\": processing_class,\n",
    "                    \"max_prompt_length\": args.max_prompt_length,\n",
    "                    \"max_completion_length\": args.max_completion_length,\n",
    "                    # for enc-dec, we add the special tokens ([bos_token] + prompt + [eos_token]; completion + [eos_token])\n",
    "                    \"add_special_tokens\": False,\n",
    "                },\n",
    "                **map_kwargs,\n",
    "            )\n",
    "            print(dataset[0])\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start DPO Training\n",
    "DPO_BS = 2\n",
    "DPO_LORA_DROPOUT = 0.1\n",
    "DPO_LORA_RANK = 16\n",
    "DPO_LORA_ALPHA = 32\n",
    "DPO_LR = \"2e-5\"\n",
    "run_name = f\"gemma-3-4b-it_r{DPO_LORA_RANK}a{DPO_LORA_ALPHA}_do01_lr{DPO_LR}_bs{DPO_BS}_epoch{DPO_EPOCH}\" + \"_dpo\"\n",
    "\n",
    "# Set up DPO configuration\n",
    "dpo_args = DPOConfig(\n",
    "    per_device_train_batch_size=DPO_BS,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=DPO_EPOCH,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    output_dir=\"dpo_results\",\n",
    "    max_length=128,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr\": 1e-8},\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=float(DPO_LR),\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=None,\n",
    "    logging_steps=1,\n",
    "    run_name=run_name,\n",
    "    # DPO specific args\n",
    "    beta=0.03,\n",
    ")\n",
    "\n",
    "# Create a new PEFT model instance for DPO training\n",
    "lora_cfg_dpo = LoraConfig(\n",
    "  r=DPO_LORA_RANK,\n",
    "  lora_alpha=DPO_LORA_ALPHA,\n",
    "  target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"gate_proj\", \"down_proj\"],\n",
    "  lora_dropout=DPO_LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Train the model with DPO\n",
    "dpo_trainer = HW7DPOTrainer(\n",
    "    model=model,\n",
    "    args=dpo_args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,  # Optional: if you want to handle tokenization\n",
    "    peft_config=lora_cfg_dpo,\n",
    ")\n",
    "\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DPO_ADAPTER_PATH = os.path.join(OUTPUT_DIR, f\"dpo_{run_name}_adapter\")\n",
    "\n",
    "dpo_trainer.save_model(DPO_ADAPTER_PATH)\n",
    "peft_model = dpo_trainer.model\n",
    "model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_model_response = []\n",
    "model.eval()\n",
    "for data in test_data:\n",
    "  id = data['food']\n",
    "  prompt = data['question']\n",
    "  print(f'\\nQuestion {id}: {prompt}')\n",
    "  inputs = data_formulate(data)\n",
    "  outputs = model.generate(\n",
    "      **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
    "      max_new_tokens = 128,\n",
    "      temperature = 0.7,\n",
    "      do_sample=False\n",
    "  )\n",
    "  output = tokenizer.batch_decode(outputs)[0]\n",
    "  output = extract_assistant_response_gemma(output)\n",
    "  print(f'Answer:{output}')\n",
    "  aligned_model_response.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24741ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model's output result\n",
    "dir_name = OUTPUT_DIR + \"hw7_dpo_results/\"\n",
    "file_name = f\"{dir_name}/hw7_epoch{DPO_EPOCH}_data_size_{data_size}set_{set_num}.json\"\n",
    "output_list = []\n",
    "\n",
    "for data, original_response, aligned_response in zip(test_data, original_model_response, aligned_model_response):\n",
    "    output_list.append({\n",
    "        \"id\": data[\"food\"],\n",
    "        \"prompt\": data[\"question\"],\n",
    "        \"preference\": data[\"preference\"],\n",
    "        \"original_response\": original_response,\n",
    "        \"aligned_response\": aligned_response\n",
    "    })\n",
    "\n",
    "output_data = {\n",
    "    \"num_epoch\": DPO_EPOCH,\n",
    "    \"data_size\": data_size,\n",
    "    \"results\": output_list\n",
    "}\n",
    "\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n file saved to {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
